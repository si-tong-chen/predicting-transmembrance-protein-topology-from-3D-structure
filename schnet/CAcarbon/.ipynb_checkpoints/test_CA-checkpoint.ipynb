{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e1259db-f48d-4399-95e4-6b86aea96a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:37:50] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule                                         <a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/features/sequence/embeddings.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">embeddings.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/features/sequence/embeddings.py#44\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">44</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         graphein.protein.features.sequence.embeddings, you need to install:   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec                                                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         biovec cannot be installed via conda                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         Alternatively, you can install graphein with the extras:              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>                                                                               <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pip install graphein<span style=\"font-weight: bold\">[</span>extras<span style=\"font-weight: bold\">]</span>                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:37:50]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule                                         \u001b]8;id=782041;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/features/sequence/embeddings.py\u001b\\\u001b[2membeddings.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=975635;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/features/sequence/embeddings.py#44\u001b\\\u001b[2m44\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         graphein.protein.features.sequence.embeddings, you need to install:   \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec                                                                \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         biovec cannot be installed via conda                                  \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         Alternatively, you can install graphein with the extras:              \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m                                                                               \u001b[2m                \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pip install graphein\u001b[1m[\u001b[0mextras\u001b[1m]\u001b[0m                                          \u001b[2m                \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:37:53] </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.visualisation, you  <a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/visualisation.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">visualisation.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/visualisation.py#35\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">35</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         need to install: pytorch3d                                         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         pytorch3d                                                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:37:53]\u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.visualisation, you  \u001b]8;id=496478;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/visualisation.py\u001b\\\u001b[2mvisualisation.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=95577;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/visualisation.py#35\u001b\\\u001b[2m35\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         need to install: pytorch3d                                         \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d    \u001b[2m                   \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         pytorch3d                                                          \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #800000; text-decoration-color: #800000\">WARNING </span> To use the Graphein submodule graphein.protein.meshes, you need to        <a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/meshes.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">meshes.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/meshes.py#29\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         install: pytorch3d                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To do so, use the following command: conda install -c pytorch3d pytorch3d <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">            </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[31mWARNING \u001b[0m To use the Graphein submodule graphein.protein.meshes, you need to        \u001b]8;id=427344;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/meshes.py\u001b\\\u001b[2mmeshes.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=430059;file:///zhome/6e/5/146981/.local/lib/python3.9/site-packages/graphein/protein/meshes.py#29\u001b\\\u001b[2m29\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         install: pytorch3d                                                        \u001b[2m            \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To do so, use the following command: conda install -c pytorch3d pytorch3d \u001b[2m            \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from task import CreateDataLabel,MapAtomNode,node_accuracy\n",
    "from schnet import SchNetModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a68fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:38:54] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Amending test_BETAdata                                                      <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">task.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py#78\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">78</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:38:54]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Amending test_BETAdata                                                      \u001b]8;id=572433;file:///work3/s194408/Project/schnet_CAcarbon/task.py\u001b\\\u001b[2mtask.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=996752;file:///work3/s194408/Project/schnet_CAcarbon/task.py#78\u001b\\\u001b[2m78\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span> examples in test_BETA                                       <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#307\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">307</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m81\u001b[0m examples in test_BETA                                       \u001b]8;id=50502;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=20760;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#307\u001b\\\u001b[2m307\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Structure Analysis Using Geometric: 100%|███████| 81/81 [00:02<00:00, 28.49it/s]\n",
      "Finding dismatch and processing ...: 100%|█████| 81/81 [00:00<00:00, 222.53it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:39:05] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Have finshed finding dismatch and processing, after processing not   <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#143\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         founded dismatch                                                     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:39:05]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Have finshed finding dismatch and processing, after processing not   \u001b]8;id=524815;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=737691;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#143\u001b\\\u001b[2m143\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         founded dismatch                                                     \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Processing test_BETA data                                                   <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">task.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py#82\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">82</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing test_BETA data                                                   \u001b]8;id=932413;file:///work3/s194408/Project/schnet_CAcarbon/task.py\u001b\\\u001b[2mtask.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=538378;file:///work3/s194408/Project/schnet_CAcarbon/task.py#82\u001b\\\u001b[2m82\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Structure Analysis Using Geometric: 100%|█████████| 3/3 [00:02<00:00,  1.05it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:39:14] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Processing test_BETA labels                                                 <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">task.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/task.py#87\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">87</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:39:14]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing test_BETA labels                                                 \u001b]8;id=630150;file:///work3/s194408/Project/schnet_CAcarbon/task.py\u001b\\\u001b[2mtask.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=947570;file:///work3/s194408/Project/schnet_CAcarbon/task.py#87\u001b\\\u001b[2m87\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/04/23 16:39:15] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> Found <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81</span> examples in test_BETA                                       <a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">data_utils.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#307\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">307</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/04/23 16:39:15]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Found \u001b[1;36m81\u001b[0m examples in test_BETA                                       \u001b]8;id=582436;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py\u001b\\\u001b[2mdata_utils.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=224296;file:///work3/s194408/Project/schnet_CAcarbon/data_utils.py#307\u001b\\\u001b[2m307\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test_BETA node label: 100%|███████████| 3/3 [00:00<00:00, 295.35it/s]\n",
      "Processing test_BETA atom label: 100%|████████████| 3/3 [00:00<00:00, 64.56it/s]\n"
     ]
    }
   ],
   "source": [
    "##['train', 'val', 'test_SP_TM', 'test_TM', 'test_BETA']\n",
    "batch_size = 32\n",
    "raw_data_name = \"DeepTMHMM.3line\"\n",
    "path ='/work3/s194408/Project/'\n",
    "processor = CreateDataLabel(path,batch_size =batch_size,raw_data_name=raw_data_name)\n",
    "# processor.initialization()# split and download trian/val/test just once\n",
    "# train_data,train_lable, train_max_len,train_real_node_label,train_CA_index_list,train_atoms_length = processor.datalabelgenerator('train')\n",
    "\n",
    "# val_data,val_lable,val_max_len,val_real_node_label,val_CA_index_list,val_atoms_length = processor.datalabelgenerator('val')\n",
    "\n",
    "# test_SP_TM_data,test_SP_TM_lable, test_SP_TM_max_len,test_SP_TM_real_node_label,test_SP_TM_CA_index_list,test_SP_TM_atoms_length = processor.datalabelgenerator('test_SP_TM')\n",
    "\n",
    "# test_TM_data,test_TM_lable,test_TM_max_len,test_TM_real_node_label,test_TM_CA_index_list,test_TM_atoms_length = processor.datalabelgenerator('test_TM')\n",
    "\n",
    "test_BETA_data,test_BETA_lable,test_BETA_max_len,test_BETA_real_node_label,test_BETA_CA_index_list,test_BETA_atoms_length = processor.datalabelgenerator('test_BETA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "381fc2ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /zhome/6e/5/146981/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/work3/s194408/Project/schnet_CAcarbon/wandb/run-20231204_164110-l9gg9or0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/transmembrane-topology/DL_tmp/runs/l9gg9or0' target=\"_blank\">batchsize=16 </a></strong> to <a href='https://wandb.ai/transmembrane-topology/DL_tmp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/transmembrane-topology/DL_tmp' target=\"_blank\">https://wandb.ai/transmembrane-topology/DL_tmp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/transmembrane-topology/DL_tmp/runs/l9gg9or0' target=\"_blank\">https://wandb.ai/transmembrane-topology/DL_tmp/runs/l9gg9or0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f96edeff880>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f96ec062a90, execution_count=4 error_before_exec=None error_in_exec=None info=<ExecutionInfo object at 7f96ec0624c0, raw_cell=\"# start a new wandb run to track this script\n",
      "wandb..\" store_history=True silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DL_tmp\", #项目名称\n",
    "    entity=\"transmembrane-topology\", # 用户名\n",
    "    group=\"CA carbon\", # 对比实验分组\n",
    "    name= \"batchsize=32 \", #实验的名字\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"schnet\",\n",
    "    \"dataset\": \"protein 3D structures \",\n",
    "    \"epochs\":100,\n",
    "    'batch_size':32,\n",
    "    'hidden_channels' :256,\n",
    "    'weight_decay': 1e-4\n",
    "\n",
    "\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a2722fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._resume_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f96edeff880>> (for pre_run_cell), with arguments args (<ExecutionInfo object at 7f9624357040, raw_cell=\"device = torch.device('cuda' if torch.cuda.is_avai..\" store_history=True silent=False shell_futures=True cell_id=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_resume_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _resume_backend() takes 1 positional argument but 2 were given"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_max_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(\u001b[43mtrain_max_len\u001b[49m,val_max_len,test_SP_TM_max_len,test_TM_max_len,test_BETA_max_len)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m#StaticEmbedding need max_len\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# put model to GPU\u001b[39;00m\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m SchNetModel(hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, out_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, max_len\u001b[38;5;241m=\u001b[39mmax_len)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_max_len' is not defined"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in callback <bound method _WandbInit._pause_backend of <wandb.sdk.wandb_init._WandbInit object at 0x7f96edeff880>> (for post_run_cell), with arguments args (<ExecutionResult object at 7f962615b100, execution_count=5 error_before_exec=None error_in_exec=name 'train_max_len' is not defined info=<ExecutionInfo object at 7f9624357040, raw_cell=\"device = torch.device('cuda' if torch.cuda.is_avai..\" store_history=True silent=False shell_futures=True cell_id=None> result=None>,),kwargs {}:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_pause_backend() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;31mTypeError\u001b[0m: _pause_backend() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "max_len=max(train_max_len,val_max_len,test_SP_TM_max_len,test_TM_max_len,test_BETA_max_len)+1 #StaticEmbedding need max_len\n",
    "# put model to GPU\n",
    "model = SchNetModel(hidden_channels=256, out_dim=6, max_len=max_len).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad572dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2eaf53-7d2e-43fb-9081-a0c731608cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=100\n",
    "draw_num = 1\n",
    "global_step = 0\n",
    "\n",
    "\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_node_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_node_level_accuracy_record_val = []\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    epoch_atom_level_accuracy_train = []\n",
    "    epoch_loss_train=[]\n",
    "    epoch_node_level_accuracy_train = []\n",
    "    # train\n",
    "    for i, data in enumerate(train_data):  \n",
    "        global_step += 1 \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(data.to(device))   # put batch data in GPU get logits\n",
    "        prediction = outputs[\"node_embedding\"]  \n",
    "        real_label = torch.argmax(torch.tensor(train_lable[i]), dim=1).to(device) # put label in GPU\n",
    "        loss = criterion(prediction, real_label)  # operate in the same device\n",
    "        loss.backward()     \n",
    "        optimizer.step()    \n",
    "\n",
    "        #calulate atom-level accuracy and \n",
    "        _, predicted = torch.max(prediction, 1) \n",
    "        correct = (predicted == real_label).sum().item()\n",
    "        total = real_label.size(0)\n",
    "        atom_level_accuracy =  correct / total\n",
    "\n",
    "        #node-level accuracy\n",
    "        j=0\n",
    "        CA_pred_all=[]\n",
    "        for k in range(len(train_atoms_length[i])):\n",
    "            index_last = int(train_atoms_length[i][k]) + int(j)\n",
    "            part_pred = predicted[j:index_last]\n",
    "            CA_pred = [part_pred[index] for index in train_CA_index_list[i][k]]\n",
    "            CA_pred_all.extend(CA_pred)\n",
    "            j = train_atoms_length[i][k]\n",
    "\n",
    "        tensor_label = torch.tensor(train_real_node_label[i], dtype=torch.float32).to(device)\n",
    "        CA_pred_all= [t.unsqueeze(0) for t in CA_pred_all]\n",
    "        CA_pred_all = torch.cat(CA_pred_all, dim=0)\n",
    "        node_correct = (CA_pred_all == tensor_label).sum().item()\n",
    "        node_total = CA_pred_all.size(0)\n",
    "        node_level_accuracy =  node_correct / node_total\n",
    "\n",
    "        wandb.log({'train_loss_step':loss.item(), 'global_step':global_step})\n",
    "        wandb.log({'train_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "        wandb.log({'train_node_level_accuracy_step':node_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "        epoch_loss_train.append(loss.item())\n",
    "        epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "        epoch_node_level_accuracy_train.append(node_level_accuracy)\n",
    "        \n",
    "    epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "    epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "    epoch_node_level_accuracy_record_train.append(np.mean(epoch_node_level_accuracy_train))\n",
    "\n",
    "    wandb.log({'train_loss_epoch':np.mean(epoch_loss_train), 'global_step':global_step})\n",
    "    wandb.log({'train_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_train),  'global_step':global_step})\n",
    "    wandb.log({'train_node_level_accuracy_epoch':np.mean(epoch_node_level_accuracy_train), 'global_step':global_step})\n",
    "    \n",
    "    # val\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "\n",
    "        epoch_atom_level_accuracy_val = []\n",
    "        epoch_loss_val = []\n",
    "        epoch_node_level_accuracy_val = []\n",
    "        \n",
    "        for i, data in enumerate(val_data):  \n",
    "            outputs = model(data.to(device))\n",
    "            prediction = outputs[\"node_embedding\"]\n",
    "            real_label = torch.argmax(torch.tensor(val_lable[i]), dim=1).to(device)\n",
    "            loss = criterion(prediction, real_label)\n",
    "\n",
    "            \n",
    "            _, predicted = torch.max(prediction, 1)\n",
    "            correct = (predicted == real_label).sum().item()\n",
    "            total = real_label.size(0)\n",
    "            atom_level_accuracy = correct / total\n",
    "\n",
    "            #node-level accuracy\n",
    "            j=0\n",
    "            CA_pred_all=[]\n",
    "            for k in range(len(val_atoms_length[i])):\n",
    "                index_last = int(val_atoms_length[i][k]) + int(j)\n",
    "                part_pred = predicted[j:index_last]\n",
    "                CA_pred = [part_pred[index] for index in val_CA_index_list[i][k]]\n",
    "                CA_pred_all.extend(CA_pred)\n",
    "                j = val_atoms_length[i][k]\n",
    "\n",
    "            tensor_label = torch.tensor(val_real_node_label[i], dtype=torch.float32).to(device)\n",
    "            CA_pred_all= [t.unsqueeze(0) for t in CA_pred_all]\n",
    "            CA_pred_all = torch.cat(CA_pred_all, dim=0)\n",
    "            node_correct = (CA_pred_all == tensor_label).sum().item()\n",
    "            node_total = CA_pred_all.size(0)\n",
    "            node_level_accuracy =  node_correct / node_total\n",
    "            \n",
    "            wandb.log({'val_loss_step':loss.item(), 'global_step':global_step})\n",
    "            wandb.log({'val_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "            wandb.log({'val_node_level_accuracy_step':node_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "\n",
    "            epoch_loss_val.append(loss.item())\n",
    "            epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "            epoch_node_level_accuracy_val.append(node_level_accuracy)\n",
    "            \n",
    "        epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "        epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "        epoch_node_level_accuracy_record_val.append(np.mean(epoch_node_level_accuracy_val))\n",
    "\n",
    "        wandb.log({'val_loss_epoch':np.mean(epoch_loss_val), 'global_step':global_step})\n",
    "        wandb.log({'val_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_val), 'global_step':global_step})\n",
    "        wandb.log({'val_node_level_accuracy_epoch':np.mean(epoch_node_level_accuracy_val), 'global_step':global_step})\n",
    "\n",
    "    if epoch % draw_num == 0:\n",
    "        print(f\"EPOCH:{epoch}:Train Loss:{np.mean(epoch_loss_train)} Train Atom Level Accuracy:{np.mean(epoch_atom_level_accuracy_train)} Train Node Level Accuracy:{np.mean(epoch_node_level_accuracy_train)}\")\n",
    "        print(f\"EPOCH:{epoch}:Val Loss:{np.mean(epoch_loss_val)} Val Atom Level Accuracy:{np.mean(epoch_atom_level_accuracy_val)} Val Node Level Accuracy:{np.mean(epoch_node_level_accuracy_val)}\")\n",
    "\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "print(\"Finished training.\")\n",
    "\n",
    "torch.save(model.state_dict(), '/work3/s194408/Project/result/final_model_CA_size32.pth')\n",
    "\n",
    "   \n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f5367d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
