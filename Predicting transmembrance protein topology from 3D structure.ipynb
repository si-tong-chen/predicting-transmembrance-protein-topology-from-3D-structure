{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting transmembrane protein topology from 3D structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1.**Protein Topology and Its Importance**\n",
    "\n",
    "Protein topology tells about the location of a particular protein part concerning a cell. A protein can be inside or outside a cell, but it can also be in-between the cell membrane as illustrated in the following Figure. A transmembrane protein might serve as a receptor for cell-to-cell communication while a free-moving protein outside or inside a cell might play a role in intracellular transportation [1]\n",
    "\n",
    "2.**Advances in Protein Topology Prediction**\n",
    "\n",
    "- Bernsel et al. (2009) utilized an ensemble of five different 1D sequence models for topology prediction [2].\n",
    "- Dobson et al. (2015) improved upon this with a similar approach, but employing ten models [3].\n",
    "- Recently, Hallgren et al. (2018) demonstrated significant performance improvements with the DeepTMHMM model, a neural network-based approach [4].\n",
    "\n",
    "3.**Our Approach: A GNN-based Method**\n",
    "\n",
    "However, existing models primarily leverage only 1D protein sequence information for topology prediction. In contrast, our study introduces a Graph Neural Network (GNN)-based method that uses 3D structural information obtained from the well-known Alphafold model developed by DeepMind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/figure1.png\" alt=\"Image\" width=\"500\" height=\"250\"></p> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset \\& 3D protein structure**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### **Protein Types and 3D Predictions Availability**\n",
    "The dataset used in this study is the same as the study behind the DeeoTMHMM model. Following table provides an overview of which protein types are included and whether or not their 3D predictions are available from the AlphaDB.\n",
    "\n",
    "\n",
    "The 3D protein structures corresponding to the sequences in Table are predicted by Alphafold and acquired from the Alphafold Data Base [4,5]. These structures are stored in pdb files, which each contains the protein 1D sequence, the atoms of the protein, the atom coordinates and the prediction uncertainties. \n",
    "\n",
    "> **Note**: TM: Transmembrane, SP: Signal Peptide. The numbers without parentheses indicate sequences with both 1D and 3D structures available.\n",
    "\n",
    "| Protein Type | alpha TM | alpha SP+TM | beta barrel | Globular   | SP+Globular |\n",
    "| ------------ | -------- | ----------- | ----------- | ---------- | ----------- |\n",
    "| Amount       | 383 (387)| 102 (106)   | 82 (82)     | 1,982 (2,000) | 994 (1,000) |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### **Graph Construction from PDB Files**\n",
    "\n",
    "A graph in the form of an adjacency matrix can then be constructed based on the atom coordinates (see block 3 in follwoing figure). The atoms include carbon (C), nitrogen (N), oxygen (O) and sulfur (S) while Hydrogen (H) is omitted due to its relatively low importance. Upon data inspection, it has been observed every residue (bound amino acid) would start with the nitrogen atom in the graph. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/figure2.png\" alt=\"Image\" width=\"500\" height=\"250\"></p> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Methodology**\n",
    "\n",
    "The output of an *invariant* GNN always has the same order while the output of an *equivariant* GNN has the same order as the input order. In this study, we applied the state-of-the-art GNNs such as **SchNet (invariant)**, **EGNN (equivariant)**, and **GCPNet (equivariant)** to the topological task. However, SchNet was the only GNN that we managed to get meaningful results from.\n",
    "\n",
    "\n",
    " The SchNet model utilizing major voting method in the downstream task  uses a 5-fold cross-validation (CV) to assess model performance, SchNet model utilizing major voting $\\alpha$-carbon method in the downstream task,EGNN and GCPNet use a 1-fold cross-validation (CV) and are compared to the SchNet model.(We utilized the identical data splits as those employed in the DeepTMHMM study for this configuration. Consequently, the model was trained on the first three sets, validated on the fourth set, and tested on the fifth set. This process was repeated five times, with the model being validated and tested on different sets each time.)\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Schent Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Architecture**\n",
    "\n",
    "1. **Input and Embedding**: \n",
    "\n",
    "    This includes representing atoms and their features (\\( t_1, ..., t_n \\)) as embedding vectors. The features of each atom can include its type, position, and other relevant properties. These features are subsequently transformed into continuous vector space (denoted as 'embedding, k') for further processing.\n",
    "\n",
    "2. **Message Passing Layers**:\n",
    "\n",
    "     These layers are responsible for encoding interactions between atoms. In each layer (labeled as 'interaction, k'), the network updates the representation of each atom by aggregating information from its neighbors. This process allows the model to capture the local chemical environment of each atom within the molecule.\n",
    "\n",
    "3. **Continuous Filter Convolution (cfconv)**: \n",
    "\n",
    "    Convolutional filters, learned during network training, are applied to interactions between atoms. The filters operate based on the distances between atoms, typically represented using radial basis functions (RBFs), and are used to model spatial relationships between atoms.\n",
    "\n",
    "4. **Nonlinear Activation (Shifted Softplus)**: \n",
    "\n",
    "    After the convolutional step, a nonlinear activation function (shifted softplus) is applied to introduce nonlinearity, enabling the model to capture more complex patterns in the data.\n",
    "\n",
    "5. **Atomic Level Layers**: \n",
    "\n",
    "    Following the convolutional and activation steps, atomic-level layers are applied. These are fully connected layers that operate independently on each atom, allowing the model to refine the representation of each atom's features.\n",
    "\n",
    "6. **Sum Pooling**: \n",
    "\n",
    "    After processing features through multiple layers, the model aggregates the features of all atoms in the molecule through sum pooling. This operation generates a global representation of the molecule for further predictions.\n",
    "\n",
    "7. **Feedforward Output Neural Network**: \n",
    "\n",
    "    The global representation is passed to a feedforward neural network to predict the target property.\n",
    "\n",
    "8. **Parameter Sharing**: \n",
    "\n",
    "    Parameters are often shared between different parts of the network (e.g., weights in convolutional filters). This sharing allows the network to generalize better and reduces the number of parameters that need to be learned.\n",
    "\n",
    "\n",
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/schnet.png\" alt=\"Image\" width=\"500\" height=\"250\"></p> [5]|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Parameter**\n",
    "\n",
    "| Model Parameter                | Value                                  | Description                                           |\n",
    "|--------------------------------|----------------------------------------|-------------------------------------------------------|\n",
    "| Number of Layers               | 6                                      | Number of layers in the SchNet model.                 |\n",
    "| Hidden Embedding Size          | 128                                    | Size of the hidden embedding for each atom.           |\n",
    "| Number of Convolutional Filters| 128                                    | Number of convolutional filters used in the model.     |\n",
    "| Maximum Number of Neighbor Nodes| 32                                    | Maximum number of neighbor nodes considered during message passing. |\n",
    "| Dropout                        | 0.8                                    | Dropout rate applied before the fully connected layers. |\n",
    "| Batch Size                     | 1 (chosen for best and stable performance) | Batch size used for training.                         |\n",
    "| Optimizer                      | Adam                                   | Optimization algorithm used for training.             |\n",
    "| Learning Rate                  | $3\\cdot 10^{-4}$                       | Learning rate for training.                           |\n",
    "| $L2$ Regularization            | $1\\cdot 10^{-4}$                       | Strength of $L2$ regularization.                     |\n",
    "| Learning Rate Scheduler        | Exponential with decay rate of 0.1     | Learning rate scheduler applied during training.      |\n",
    "| StaticEmbedding length         | 30000                                  | Length of static embedding.                           |\n",
    "| Gaussian filter during the training and validation | Kernel size = 29 and $\\sigma$ = 5 | Parameters for Gaussian smoothing during training and validation. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downstream Tasks**\n",
    "\n",
    "For our classification task predicting transmembrane proteins, we have a total of six categories.(The residue classes are *Signal peptide (S)* *Inside cytosol (I)* *Alpha membrane part (M)* *Beta membrane part (B)* *Periplasm (P)* *Outside cell (O)*) Thus, we have added two fully connected layers after the SCHNET model. The activation function employed is ReLU, and the output size is set to 6, corresponding to the number of classification categories. Furthermore, we have applied Gaussian Smoothing to the output (the `GaussianSmoothing` part is included in `from task import GaussianSmoothing`).\n",
    "\n",
    "##### **Comparing with Baseline mode**\n",
    "The model performance was compared to a baseline classifier, which predicted all test observations as belonging to the most frequently appearing class in the training set.We observed that in all the CV folds the class inside cytosol (I) was the most frequently appearing class and the class distribution was very similar for all the CV folds. The comparison was conducted using McNemars test [6]. The below pictures shows the distribution of all the label classes in the first fold setup and the distributions in the other CV folds are very similar (not shown)\n",
    "\n",
    "\n",
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/setup1dis.png\" alt=\"Image\" width=\"500\" height=\"333\"></p> \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### **Label Alignment for Major Voting**\n",
    "\n",
    "Given that the model's input originates from the Alphafold model developed by DeepMind—which cannot predict protein structures with complete accuracy—there are disparities between the model's predicted labels (Alphafold's entire sequence of predicted atoms) and the original labels (from the actual protein atomic sequences). One of the downstream tasks is, therefore, to align these labels.\n",
    "\n",
    "Label Alignment Strategies are as follow:\n",
    "\n",
    "1. **First Approach**: Keep the predicted labels static and process the real labels to facilitate the backward propagation process. This step is implemented in `from data_utils import DismatchIndexPadRawData`. The primary idea is to compare the two sets of labels one by one, identifying the index corresponding to the actual label. If the actual label is greater than the predicted label, the index corresponding to the real label is deleted. Conversely, if the actual label is smaller, the value of the predicted label is inserted at the index of the actual label. The type of transmembrane protein at that position is inferred based on the indices immediately preceding and following the true label.\n",
    "\n",
    "2. **Second Approach**: Maintain the actual labels unchanged and process the predicted labels to achieve atomic-level accuracy. Following the indices obtained from the first approach, if the actual label is larger than the predicted label, the corresponding value is inserted into the predicted label. If the actual label is smaller, the corresponding value in the predicted label is removed.\n",
    "\n",
    "3. **Third Approach**: Based on the second strategy, the predicted labels are segmented according to the length of atoms within each amino acid (proteins are composed of amino acids, which in turn consist of multiple atoms). After segmentation, a majority voting technique is used to determine the category of each atom within the predicted amino acid. The transmembrane category of the amino acid is decided by the majority vote, achieving residue-level accuracy (amino acid level). This part of the process is covered in `from task import MapAtomNode`.\n",
    "\n",
    "##### **Label Alignment for $\\alpha$-carbon**\n",
    "1. **First Approach**: This part is the same as in Label Alignment for Major Voting\n",
    "\n",
    "2. **Second Approach**: This part is the same as in Label Alignment for Major Voting\n",
    "\n",
    "\n",
    "3. **Third Approach**:Another approach is to train the GNN using the $\\alpha$-carbon embedding from each residue[7] . An $\\alpha$-carbon is the central atom linking to an amino group and a carboxyl group within an amino acid[1] . In this way, the output dimension based on the $\\alpha$-carbons would have the same dimension as the protein sequence. However, we only achieved similar or worse performance by using $\\alpha$-carbons compared to using the aforementioned major voting approach.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import CreateDataLabel,MapAtomNode,node_accuracy\n",
    "from schnet import SchNetModel\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training and Model Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Results from SCHNET Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Major Voting(5-fold cross-validation)**\n",
    "\n",
    "\n",
    "\n",
    "|<img src=\"image/schnet_loss.png\" alt=\"Image 1\" width=\"400\" height=\"200\"> | <img src=\"image/schnet_acc.png\" alt=\"Image 2\" width=\"400\" height=\"200\">|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Result:**\n",
    "\n",
    "- The residue level accuracies are evaluated by directly comparing them to the class labels. It can be seen that all the loss and accuracy curves have similar tendencies, indicating that the variability is small in each CV setup.\n",
    "\n",
    "- The abrupt jump around epoch 100 is caused by the exponential decaying learning scheduler. Please notice that the training for CV setup 5 was early stopped around epoch 90."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**$\\alpha$-carbon(1-fold cross_validation)**\n",
    "\n",
    "\n",
    "|<img src=\"image/schnetCA_loss.png\" alt=\"Image 1\" width=\"400\" height=\"200\"> | <img src=\"image/schnetCA_acc.png\" alt=\"Image 2\" width=\"400\" height=\"200\">|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 1. **Result:**\n",
    "    \n",
    "- While the loss curves converge at a lower value compared to the major voting approach, the actual correctly predicted residue labels are not better. However, this method might still be useful in improving the overall topological prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training Techniques and Strategies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **1.EarlyStopping and Learning Rate Scheduling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping:\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss >= (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "# Learning Rate Scheduling\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.GaussianSmoothing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianSmoothing(nn.Module):\n",
    "    \"\"\"\n",
    "    Apply gaussian smoothing on a\n",
    "    1d, 2d or 3d tensor. Filtering is performed seperately for each channel\n",
    "    in the input using a depthwise convolution.\n",
    "    Arguments:\n",
    "        channels (int, sequence): Number of channels of the input tensors. Output will\n",
    "            have this number of channels as well.\n",
    "        kernel_size (int, sequence): Size of the gaussian kernel.\n",
    "        sigma (float, sequence): Standard deviation of the gaussian kernel.\n",
    "        dim (int, optional): The number of dimensions of the data.\n",
    "            Default value is 2 (spatial).\n",
    "    \"\"\"\n",
    "    def __init__(self, channels, kernel_size, sigma, dim=2):\n",
    "        super(GaussianSmoothing, self).__init__()\n",
    "        if isinstance(kernel_size, numbers.Number):\n",
    "            kernel_size = [kernel_size] * dim\n",
    "        if isinstance(sigma, numbers.Number):\n",
    "            sigma = [sigma] * dim\n",
    "\n",
    "        # The gaussian kernel is the product of the\n",
    "        # gaussian function of each dimension.\n",
    "        kernel = 1\n",
    "        meshgrids = torch.meshgrid(\n",
    "            [\n",
    "                torch.arange(size, dtype=torch.float32)\n",
    "                for size in kernel_size\n",
    "            ]\n",
    "        )\n",
    "        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):\n",
    "            mean = (size - 1) / 2\n",
    "            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * \\\n",
    "                      torch.exp(-((mgrid - mean) / std) ** 2 / 2)\n",
    "\n",
    "        # Make sure sum of values in gaussian kernel equals 1.\n",
    "        kernel = kernel / torch.sum(kernel)\n",
    "\n",
    "        # Reshape to depthwise convolutional weight\n",
    "        kernel = kernel.view(1, 1, *kernel.size())\n",
    "        kernel = kernel.repeat(channels, *[1] * (kernel.dim() - 1))\n",
    "\n",
    "        self.register_buffer('weight', kernel)\n",
    "        self.groups = channels\n",
    "\n",
    "        if dim == 1:\n",
    "            self.conv = F.conv1d\n",
    "        elif dim == 2:\n",
    "            self.conv = F.conv2d\n",
    "        elif dim == 3:\n",
    "            self.conv = F.conv3d\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                'Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        Apply gaussian filter to input.\n",
    "        Arguments:\n",
    "            input (torch.Tensor): Input to apply gaussian filter on.\n",
    "        Returns:\n",
    "            filtered (torch.Tensor): Filtered output.\n",
    "        \"\"\"\n",
    "        return self.conv(input, weight=self.weight, groups=self.groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Major Voting(5-fold cross-validation)**\n",
    "\n",
    "we just show that how to run *setup1* in the follow code, from setup2 to setup5 are the same as setup1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_data_generator(cv0,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label):\n",
    "  for cv0 in cv_list:\n",
    "    cv0_data = []\n",
    "    cv0_label = []\n",
    "    cv0_batchname = []\n",
    "    cv0_dismatch_index_pred = {}\n",
    "    cv0_dismatch_index_type = {}\n",
    "    cv0_real_node_label = []\n",
    "    cv0_index = []\n",
    "    cv0_df = pd.DataFrame(columns=total_df.columns)\n",
    "    # The batch names have the same order as the data\n",
    "    for i in range(0, len(cv0)):\n",
    "      try:\n",
    "        cv0_batchname.append([total_df.index[total_df['uniprot_id_low'] == cv0[i]['id'].lower()].tolist()[0].lower()])\n",
    "      except:\n",
    "        pass\n",
    "  # Find index for the found cv0 proteins\n",
    "  # Note that the labels are aligned with the batch names\n",
    "    for i in range(0, len(cv0)):\n",
    "      try:\n",
    "        cv0_index.append(total_batchname.index([cv0[i]['id'].lower()]))\n",
    "      except:\n",
    "        pass\n",
    "    # gather the data for cv0\n",
    "    for i in range(0, len(cv0_index)):\n",
    "      cv0_data.append(total_data[cv0_index[i]])\n",
    "      cv0_label.append(total_label[cv0_index[i]])\n",
    "      cv0_dismatch_index_pred[list(total_dismatch_index_pred)[cv0_index[i]]] = list(total_dismatch_index_pred.values())[cv0_index[i]]\n",
    "      cv0_dismatch_index_type[list(total_dismatch_index_type)[cv0_index[i]]] = list(total_dismatch_index_type.values())[cv0_index[i]]\n",
    "      cv0_real_node_label.append(total_real_node_label[cv0_index[i]])\n",
    "\n",
    "      #cv0_df = pd.concat([cv0_df, total_df.loc[[total_df.iloc[cv0_index[i]][\"uniprot_id\"]]]], ignore_index=False)\n",
    "      cv0_df = pd.concat([cv0_df, total_df.loc[[cv0_batchname[i][0].upper()]]], ignore_index=False)\n",
    "    return cv0_data, cv0_label, cv0_batchname, cv0_dismatch_index_pred, cv0_dismatch_index_type, cv0_real_node_label, cv0_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "raw_data_name = \"DeepTMHMM.3line\"\n",
    "path ='/work3/s194408/Project/'\n",
    "processor = CreateDataLabel(path,batch_size =batch_size,raw_data_name=raw_data_name)\n",
    "# processor.initialization()# split and download trian/val/test just once\n",
    "train_data,train_lable, train_batchname, train_max_len,train_dismatch_index_pred,train_dismatch_index_type,train_real_node_label,df_train = processor.datalabelgenerator('train')\n",
    "val_data,val_lable, val_batchname, val_max_len,val_dismatch_index_pred,val_dismatch_index_type,val_real_node_label,df_val = processor.datalabelgenerator('val')\n",
    "test_SP_TM_data,test_SP_TM_lable, test_SP_TM_batchname, test_SP_TM_max_len,test_SP_TM_dismatch_index_pred,test_SP_TM_dismatch_index_type,test_SP_TM_real_node_label,df_test_SP_TM = processor.datalabelgenerator('test_SP_TM')\n",
    "test_TM_data,test_TM_lable, test_TM_batchname, test_TM_max_len,test_TM_dismatch_index_pred,test_TM_dismatch_index_type,test_TM_real_node_label,df_test_TM = processor.datalabelgenerator('test_TM')\n",
    "test_BETA_data,test_BETA_lable, test_BETA_batchname, test_BETA_max_len,test_BETA_dismatch_index_pred,test_BETA_dismatch_index_type,test_BETA_real_node_label,df_test_BETA = processor.datalabelgenerator('test_BETA')\n",
    "# Opening JSON file\n",
    "f = open('/work3/s194408/Project/dataset/tmp/DeepTMHMM.partitions.json')\n",
    "# returns JSON object as\n",
    "cv_data = json.load(f)\n",
    "cv0 = cv_data['cv0']\n",
    "cv1 = cv_data['cv1']\n",
    "cv2 = cv_data['cv2']\n",
    "cv3 = cv_data['cv3']\n",
    "cv4 = cv_data['cv4']\n",
    "# Group the data together\n",
    "total_data = train_data.copy()\n",
    "total_label = train_lable.copy()\n",
    "total_batchname = train_batchname.copy()\n",
    "total_max_len = train_max_len + val_max_len + test_SP_TM_max_len + test_TM_max_len + test_BETA_max_len\n",
    "total_dismatch_index_pred = train_dismatch_index_pred.copy()\n",
    "total_dismatch_index_type = train_dismatch_index_type.copy()\n",
    "total_real_node_label = train_real_node_label.copy()\n",
    "frames = [df_train, df_val, df_test_SP_TM, df_test_TM, df_test_BETA]\n",
    "total_df = pd.concat(frames)\n",
    "data_list=['val_data','test_SP_TM_data','test_TM_data','test_BETA_data']\n",
    "for name in data_list:\n",
    "  for i in range(0, len(name)):\n",
    "    total_data.append(val_data[i])\n",
    "    total_label.append(val_lable[i])\n",
    "    total_batchname.append(val_batchname[i])\n",
    "    total_dismatch_index_pred[list(val_dismatch_index_pred)[i]] = list(val_dismatch_index_pred.values())[i]\n",
    "    total_dismatch_index_type[list(val_dismatch_index_type)[i]] = list(val_dismatch_index_type.values())[i]\n",
    "    total_real_node_label.append(val_real_node_label[i]) \n",
    "cv_list = ['cv0', 'cv1', 'cv2', 'cv3', 'cv4'] \n",
    "cv0_data, cv0_label, cv0_batchname, cv0_dismatch_index_pred, cv0_dismatch_index_type, cv0_real_node_label, cv0_df=cv_data_generator(cv0,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label)\n",
    "cv1_data, cv1_label, cv1_batchname, cv1_dismatch_index_pred, cv1_dismatch_index_type, cv1_real_node_label, cv1_df=cv_data_generator(cv1,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label)\n",
    "cv2_data, cv2_label, cv2_batchname, cv2_dismatch_index_pred, cv2_dismatch_index_type, cv2_real_node_label, cv2_df=cv_data_generator(cv2,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label)\n",
    "cv3_data, cv3_label, cv3_batchname, cv3_dismatch_index_pred, cv3_dismatch_index_type, cv3_real_node_label, cv3_df=cv_data_generator(cv3,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label)\n",
    "setup1_test_data, setup1_test_label, setup1_test_batchname, setup1_test_dismatch_index_pred, setup1_test_dismatch_index_type, setup1_test_real_node_label, setup1_test_df=cv_data_generator(cv4,cv_list,total_df,total_data,total_label,total_batchname,total_dismatch_index_pred,total_dismatch_index_type,total_real_node_label)\n",
    "# CV setup 1\n",
    "#cv0, cv1, cv2 for train, cv3 for validation, cv4 for test\n",
    "setup1_train_data = cv0_data.copy()\n",
    "setup1_train_label = cv0_label.copy()\n",
    "setup1_train_batchname = cv0_batchname.copy()\n",
    "setup1_train_dismatch_index_pred = cv0_dismatch_index_pred.copy()\n",
    "setup1_train_dismatch_index_type = cv0_dismatch_index_type.copy()\n",
    "setup1_train_real_node_label = cv0_real_node_label.copy()\n",
    "setup1_train_df = [cv0_df, cv1_df, cv2_df]\n",
    "setup1_train_df = pd.concat(setup1_train_df)\n",
    "cv_train_list=['cv1','cv2']\n",
    "for name in cv_train_list:\n",
    "  for i in range(0, len(cv1_data)):\n",
    "    setup1_train_data.append(cv1_data[i])\n",
    "    setup1_train_label.append(cv1_label[i])\n",
    "    setup1_train_batchname.append(cv1_batchname[i])\n",
    "    setup1_train_dismatch_index_pred[list(cv1_dismatch_index_pred)[i]] = list(cv1_dismatch_index_pred.values())[i]\n",
    "    setup1_train_dismatch_index_type[list(cv1_dismatch_index_type)[i]] = list(cv1_dismatch_index_type.values())[i]\n",
    "    setup1_train_real_node_label.append(cv1_real_node_label[i])\n",
    "\n",
    "device = torch.device('cuda')\n",
    "# # put model to GPU\n",
    "model = SchNetModel(hidden_channels=128, out_dim=6, max_len=30000, max_num_neighbors=16).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0004,weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.05) # Learning schedule added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=100\n",
    "draw_num = 1\n",
    "global_step = 0\n",
    "# setup 1\n",
    "# cv0, cv1, cv2 for train, cv3 for validation, cv4 for test\n",
    "# the training data is called setup1_train_data\n",
    "early_stopper = EarlyStopper(patience=5, min_delta=0.001) # se min uprise\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_node_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_node_level_accuracy_record_val = []\n",
    "epochs = []\n",
    "for epoch in range(total_epochs):\n",
    "    epochs.append(epoch)\n",
    "    epoch_atom_level_accuracy_train = []\n",
    "    epoch_loss_train=[]\n",
    "    epoch_node_level_accuracy_train = []\n",
    "    # train\n",
    "    for i, data in enumerate(setup1_train_data):  \n",
    "        global_step += 1 \n",
    "        optimizer.zero_grad()  \n",
    "        outputs = model(data.to(device))   # put batch data in GPU get logits\n",
    "        prediction = outputs[\"node_embedding\"]  \n",
    "        real_label = torch.argmax(torch.tensor(setup1_train_label[i]), dim=1).to(device) # put label in GPU  \n",
    "           \n",
    "        # Apply Gaussian smoothing before backpropagation\n",
    "        prediction_Gauss = torch.from_numpy(gaussian_filter1d(prediction.cpu().detach().numpy(), 1, radius=3)).clone().detach().requires_grad_(True)\n",
    "        \n",
    "        loss = criterion(prediction_Gauss.to(device), real_label)\n",
    "        loss.backward()     \n",
    "        optimizer.step()    \n",
    "        #calulate atom-level accuracy and node-level accuracy\n",
    "        _, predicted = torch.max(prediction_Gauss.to(device), 1)\n",
    "        correct = (predicted == real_label).sum().item()\n",
    "        total = real_label.size(0)\n",
    "        atom_level_accuracy =  correct / total\n",
    "        # below is operated under CPU node\n",
    "        processor = MapAtomNode(predicted.cpu(),setup1_train_batchname[i],setup1_train_dismatch_index_pred,setup1_train_dismatch_index_type,setup1_train_df)\n",
    "        train_predict_node_label = processor.map_atom_node() \n",
    "        node_level_accuracy = node_accuracy(train_predict_node_label,setup1_train_real_node_label[i])\n",
    "        epoch_loss_train.append(loss.item())\n",
    "        epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "        epoch_node_level_accuracy_train.append(node_level_accuracy)    \n",
    "    epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "    epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "    epoch_node_level_accuracy_record_train.append(np.mean(epoch_node_level_accuracy_train))    \n",
    "    # val\n",
    "    model.eval()  \n",
    "    with torch.no_grad():  \n",
    "        epoch_atom_level_accuracy_val = []\n",
    "        epoch_loss_val = []\n",
    "        epoch_node_level_accuracy_val = []\n",
    "        for i, data in enumerate(cv3_data):  \n",
    "            outputs = model(data.to(device))\n",
    "            prediction = outputs[\"node_embedding\"]\n",
    "            real_label = torch.argmax(torch.tensor(cv3_label[i]), dim=1).to(device)\n",
    "            # Apply Gaussian smoothing before back propagation\n",
    "            prediction_Gauss = torch.from_numpy(gaussian_filter1d(prediction.cpu().detach().numpy(), 1, radius=3)).clone().detach().requires_grad_(True)\n",
    "            loss = criterion(prediction_Gauss.to(device), real_label)\n",
    "            #_, predicted = torch.max(prediction, 1)\n",
    "            _, predicted = torch.max(prediction_Gauss.to(device), 1)\n",
    "            correct = (predicted == real_label).sum().item()\n",
    "            total = real_label.size(0)\n",
    "            atom_level_accuracy = correct / total\n",
    "            processor = MapAtomNode(predicted.cpu(), cv3_batchname[i], cv3_dismatch_index_pred, cv3_dismatch_index_type, cv3_df)\n",
    "            val_predict_node_label = processor.map_atom_node()\n",
    "            node_level_accuracy = node_accuracy(val_predict_node_label, cv3_real_node_label[i])\n",
    "            epoch_loss_val.append(loss.item())\n",
    "            epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "            epoch_node_level_accuracy_val.append(node_level_accuracy)\n",
    "            epoch_loss_val.append(loss.item())\n",
    "            epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "            epoch_node_level_accuracy_val.append(node_level_accuracy)\n",
    "        epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "        epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "        epoch_node_level_accuracy_record_val.append(np.mean(epoch_node_level_accuracy_val))\n",
    "        # early_stopping needs the validation loss to check if it has decresed, \n",
    "        # and if it has, it will make a checkpoint of the current model\n",
    "        if early_stopper.early_stop(np.mean(epoch_loss_val)):             \n",
    "            break\n",
    "        if global_step >= 55000:\n",
    "            scheduler.step() # apply learning schedule\n",
    "torch.save(model.state_dict(), '/work3/s194408/Project/result/CV_setup1_5_neighbors_Gauss.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **$\\alpha$-carbon(1-fold cross_validation)**\n",
    "replace MapAtomNode part with follow coding to achieve the methond of $\\alpha$-carbon\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node-level accuracy\n",
    "j=0\n",
    "CA_pred_all=[]\n",
    "for k in range(len(setup_val_total_atoms_length[i])):\n",
    "    index_last = int(setup_val_total_atoms_length[i][k]) + int(j)\n",
    "    part_pred = predicted[j:index_last]\n",
    "    CA_pred = [part_pred[index] for index in setup_val_CA_index_list[i][k]]\n",
    "    CA_pred_all.extend(CA_pred)\n",
    "    j = setup_val_total_atoms_length[i][k]\n",
    "\n",
    "tensor_label = torch.tensor(setup_val_real_node_label[i], dtype=torch.float32).to(device)\n",
    "CA_pred_all= [t.unsqueeze(0) for t in CA_pred_all]\n",
    "CA_pred_all = torch.cat(CA_pred_all, dim=0)\n",
    "node_correct = (CA_pred_all == tensor_label).sum().item()\n",
    "node_total = CA_pred_all.size(0)\n",
    "node_level_accuracy =  node_correct / node_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Test and Result**\n",
    "\n",
    "This part is about **Major Voting(5-fold cross-validation)**,the result of **$\\alpha$-carbon(1-fold cross_validation)** represnts in the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### **1.Topological evaluation**\n",
    "\n",
    "The evaluation criteria for two identical protein topologies are defined according to[4] as follows:\n",
    "\n",
    "- The N-terminal topology must be the same.\n",
    "- The predicted labels and the ground truth labels must overlap with 5 residues for α-helices and 2 for β-strands (sub-parts of β-barrel).\n",
    "\n",
    "\n",
    "The test results are gathered, so the topological predictions are evaluated using the above-mentioned criteria for the protein types alpa TM, alpa SP+TM, and beta barrel. \n",
    "In ***Left Table***, it can be seen that only very few topologies have been predicted for *Globular* and *SP+Globular* by the trained SchNet models. However, the average correctly predicted residues pr. protein is still much higher for SchNet than for the baseline. The average correctly predicted residues pr. protein is also calculated from DeepTMHMM predictions for reference.Please notice that the overlap criterion is set to be 5 by default for the topological evaluation of Globular and SP+Globular as these proteins contain both α-helices and β-sheetsit can be observed that neither the trained SchNet models nor the baseline have predicted any topology. However, the total numbers of matched residue predictions are still much higher for SchNet than for the baseline.\n",
    "\n",
    "\n",
    "##### **2.Comparison between baseline and SchNet using McNemars test**\n",
    "\n",
    "To assess the validity of our model, we conducted a comparison between the baseline and our trained SchNet models using the McNemar's test. ***Right Table*** provides the comparison results for each of the 5-fold cross-validation (CV) setups. It is evident that the value 0 consistently falls outside the 95% confidence intervals (CIs), and the p-values are consistently highly significant. This means that the\n",
    "SchNet models are better for the topological prediction than the baseline\n",
    "\n",
    "Finally, it is noteworthy that we initially used a hold-out approach to train and validate the GNN model on only Globular and SP+Globular proteins and tested on the remaining three protein types. However, the result was even worse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "|<img src=\"image/table1.png\" alt=\"Image 1\" width=\"400\" height=\"200\"> | <img src=\"image/table3.png\" alt=\"Image 2\" width=\"400\" height=\"200\">|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we just show that how to run *setup1* test in the follow code, from setup2 test to setup5 test are the same as setup1 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SchNetModel(hidden_channels=128, out_dim=6, max_len=30000, max_num_neighbors=32).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0003,weight_decay=1e-4)\n",
    "check_point = torch.load('/content/drive/MyDrive/02456 Deep learning/Model evaluation/schnet_majorvoting/models/CV_setup1_Gauss_result.pth', map_location=torch.device('cuda'))\n",
    "model.load_state_dict(check_point)\n",
    "setup1_test_predict_node_label_lis = []\n",
    "check_zero = []\n",
    "test_node_acc_list = []\n",
    "test_node_acc_binary_list = []\n",
    "test_atom_acc_list = []\n",
    "test_atom_correct = []\n",
    "test_atom_total = []\n",
    "baseline_atom_correct = []\n",
    "baseline_atom_acc_list = []\n",
    "baseline_node_acc_list = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "  for i, data in enumerate(setup1_test_data):\n",
    "      torch.no_grad()\n",
    "      outputs = model(data.to(device))\n",
    "      prediction = outputs[\"node_embedding\"]\n",
    "      real_label = torch.argmax(torch.tensor(setup1_test_label[i]), dim=1).to(device)\n",
    "      smoothing = GaussianSmoothing(6, 29, 5, 1)\n",
    "      predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "      predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "      predicted = smoothing(predicted)\n",
    "      prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "      loss = criterion(prediction_Gauss.to(device), real_label)\n",
    "      _, predicted = torch.max(prediction_Gauss.to(device), 1)\n",
    "      correct = (predicted == real_label).sum().item()\n",
    "      total = real_label.size(0)\n",
    "      atom_level_accuracy = correct / total\n",
    "      test_atom_acc_list.append(atom_level_accuracy)\n",
    "      test_atom_correct.append(correct)\n",
    "      test_atom_total.append(total)\n",
    "      # Baseline atom accuracy\n",
    "      baseline_atom = torch.zeros_like(real_label) # note that in this case the most frequent class is class 0\n",
    "      baseline_correct = (baseline_atom == real_label).sum().item()\n",
    "      baseline_atom_correct.append(baseline_correct)\n",
    "      processor = MapAtomNode(predicted.cpu(), setup1_test_batchname[i], setup1_test_dismatch_index_pred, setup1_test_dismatch_index_type, setup1_test_df)\n",
    "      test_predict_node_label = processor.map_atom_node()\n",
    "      setup1_test_predict_node_label_lis.append(test_predict_node_label)\n",
    "      accuracy_list = [1 if x == y else 0 for x, y in zip(test_predict_node_label, setup1_test_real_node_label[i])]\n",
    "      test_node_acc_binary_list += accuracy_list\n",
    "      node_level_accuracy = node_accuracy(test_predict_node_label, setup1_test_real_node_label[i])\n",
    "      test_node_acc_list.append(node_level_accuracy)\n",
    "\n",
    "      # Baseline node accuracy\n",
    "      baseline_node_label = np.zeros_like(np.array(setup1_test_real_node_label[i])) # note that in this case the most frequent class is class 0\n",
    "      baseline_accuracy = [1 if x == y else 0 for x, y in zip(baseline_node_label, setup1_test_real_node_label[i])]\n",
    "      baseline_node_acc_list.append(baseline_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from metrics_utils import label_list_to_topology,is_topologies_equal\n",
    "print(len(setup1_test_predict_node_label_lis))\n",
    "print(len(setup1_test_real_node_label))\n",
    "final_node_acc = sum(test_node_acc_list)/len(test_node_acc_list)\n",
    "print(\"Node acc:\", final_node_acc)\n",
    "final_node_binary_acc = sum(test_node_acc_binary_list)/len(test_node_acc_binary_list)\n",
    "print(\"Node binary acc:\", final_node_binary_acc)\n",
    "final_atom_acc = sum(test_atom_acc_list)/len(test_atom_acc_list)\n",
    "print(\"Avg atom acc:\", final_atom_acc)\n",
    "total_atom_acc = sum(test_atom_correct)/len(test_atom_total)\n",
    "print(\"Total atom acc:\", final_atom_acc)\n",
    "test_resul = []\n",
    "test_baseline = []\n",
    "for i in range(0, len(setup1_test_predict_node_label_lis)):\n",
    "  topo_A = label_list_to_topology(setup1_test_predict_node_label_lis[i])\n",
    "  topo_B = label_list_to_topology(setup1_test_real_node_label[i])\n",
    "  topo_baseline = baseline_node_acc_list[i]\n",
    "  test_resul.append(is_topologies_equal(topo_A, topo_B, 5))\n",
    "  test_baseline.append(is_topologies_equal(topo_A, topo_baseline, 5))\n",
    "print(\"Correct topology:\", sum(test_resul)/len(test_resul))\n",
    "print(\"Correct topology baseline:\", sum(test_baseline)/len(test_baseline))\n",
    "baseline_all = []\n",
    "for i in range(0, len(baseline_node_acc_list)):\n",
    "  baseline_all += baseline_node_acc_list[i]\n",
    "final_node_acc = sum(baseline_all )/len(baseline_all)\n",
    "print(\"Baseline Node acc:\", final_node_acc)\n",
    "final_atom_acc = sum(baseline_atom_correct)/sum(test_atom_total)\n",
    "print(\"Baseline Atom acc:\", final_atom_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "# Compute the Jeffreys interval\n",
    "baseline_node_acc_array = np.array(baseline_all)\n",
    "test_node_acc_array = np.array(test_node_acc_binary_list)\n",
    "alpha = 0.05\n",
    "[thetahat, CI, p] = mcnemar(np.ones_like(baseline_node_acc_array), baseline_node_acc_array, test_node_acc_array, alpha=alpha)\n",
    "print(\"theta = theta_A-theta_B point estimate\", thetahat, \" CI: \", CI, \"p-value\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **EGNN Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Architecture**\n",
    "\n",
    "The core advantage of EGNN (Equivariant Graph Neural Network) is its ability to maintain feature invariance and equivariance. This ensures that regardless of the input order of the graphs, the model's output remains consistent, which is particularly crucial for learning the structural characteristics of protein 3D structures. The following picture show the basic workflow.\n",
    "\n",
    "\n",
    "1. **Graph Construction**\n",
    "  - Node Initialization: Node features are initialized through an embedding layer (EMB). This involves transforming the raw data (the 3D structure of the protein) using `torch_geometric` into a feature representation that is suitable for neural network processing.\n",
    "\n",
    "  - Edge Initialization: Similarly, edge features are initialized and transformed via `torch_geometric`, defining the relationships and interactions between nodes.\n",
    "\n",
    "2. **EGNN: Node Feature Update**\n",
    "  - Aggregation: Each node's features are aggregated by considering the features of adjacent nodes, facilitating the propagation of information across the nodes.\n",
    "\n",
    "  - Transformation: The aggregated features undergo further processing through a transformation layer (TRANS) to learn complex feature representations.\n",
    "\n",
    "  - Metric: The similarity between nodes, post feature update, is calculated and used to guide the classification task for protein transmembrane domains.\n",
    "\n",
    "3. **EGNN: Edge Feature Update**  \n",
    "  - Following the node feature update, edge features are also updated to capture the refined relationships between nodes.\n",
    "\n",
    "4. **Query Node Label Prediction**\n",
    "  - The model concludes by predicting labels for query nodes, an essential step for tasks such as protein classification.\n",
    "\n",
    "\n",
    "\n",
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/egnn.png\" alt=\"Image\" width=\"500\" height=\"250\"></p>|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downstream Tasks**\n",
    "For our classification task predicting transmembrane proteins, we have a total of six categories. Thus, we have added two fully connected layers after the EGNN model. The activation function employed is ReLU, and the output size is set to 6, corresponding to the number of classification categories. Furthermore, we have applied Gaussian Smoothing to the output (the `GaussianSmoothing` part is included in `from task import GaussianSmoothing`).\n",
    "\n",
    "#### **Label Alignment for Downstream Tasks**\n",
    "\n",
    "Given that the model's input originates from the Alphafold model developed by DeepMind—which cannot predict protein structures with complete accuracy—there are disparities between the model's predicted labels (Alphafold's entire sequence of predicted atoms) and the original labels (from the actual protein atomic sequences). One of the downstream tasks is, therefore, to align these labels.\n",
    "\n",
    "Label Alignment Strategies are as follow:\n",
    "\n",
    "1. **First Approach**: Keep the predicted labels static and process the real labels to facilitate the backward propagation process. This step is implemented in `From data_utils import DismatchIndexPadRawData`. The primary idea is to compare the two sets of labels one by one, identifying the index corresponding to the actual label. If the actual label is greater than the predicted label, the index corresponding to the real label is deleted. Conversely, if the actual label is smaller, the value of the predicted label is inserted at the index of the actual label. The type of transmembrane protein at that position is inferred based on the indices immediately preceding and following the true label.\n",
    "\n",
    "2. **Second Approach**: Maintain the actual labels unchanged and process the predicted labels to achieve atomic-level accuracy. Following the indices obtained from the first approach, if the actual label is larger than the predicted label, the corresponding value is inserted into the predicted label. If the actual label is smaller, the corresponding value in the predicted label is removed.\n",
    "\n",
    "3. **Third Approach**: Based on the second strategy, the predicted labels are segmented according to the length of atoms within each amino acid (proteins are composed of amino acids, which in turn consist of multiple atoms). After segmentation, a majority voting technique is used to determine the category of each atom within the predicted amino acid. The transmembrane category of the amino acid is decided by the majority vote, achieving residue-level accuracy (amino acid level). This part of the process is covered in `from task import MapAtomNode`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Parameter**\n",
    "\n",
    "Due to the variable lengths of the primary structures' atomic sequences in proteins, we employed the StaticEmbedding method for embedding node features. Additionally, we set the input length to `20000 * batch size`.\n",
    "\n",
    "\n",
    "| Parameter Name              | Value       | Description                               |\n",
    "| --------------------------- | ----------- | ----------------------------------------- |\n",
    "| `num_layers`                | 5           | Number of layers in the model.           |\n",
    "| `emb_dim`                   | 128         | Embedding dimension for the model.       |\n",
    "| `Activation function to use`| relu        | Activation function used in the model.   |\n",
    "| `norm`                      | layer       | Normalization method used (e.g., \"layer\").|\n",
    "| `aggr`                      | mean        | Aggregation method (e.g., \"mean\").       |\n",
    "| `learning rate`             | 0.01        | Learning rate for training.              |\n",
    "| `weight_decay`              | 1e-4        | Weight decay regularization term.        |\n",
    "| `batch size`                | 1           | Batch size for training.                 |\n",
    "| `pool`                      | mean        | Pooling method used (e.g., \"mean\").      |\n",
    "| `residual`                  | True        | Whether residual connections are used.   |\n",
    "| `dropout`                   | 0.1         | Dropout rate for regularization.         |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downstream Tasks**\n",
    "\n",
    "For our classification task predicting transmembrane proteins, we have a total of six categories. Thus, we have added two fully connected layers after the EGNN model. The activation function employed is ReLU, and the output size is set to 6, corresponding to the number of classification categories. Furthermore, we have applied Gaussian Smoothing to the output (the `GaussianSmoothing` part is included in `from task import GaussianSmoothing`).\n",
    "\n",
    "##### Label Alignment for Downstream Tasks\n",
    "\n",
    "Given that the model's input originates from the Alphafold model developed by DeepMind—which cannot predict protein structures with complete accuracy—there are disparities between the model's predicted labels (Alphafold's entire sequence of predicted atoms) and the original labels (from the actual protein atomic sequences). One of the downstream tasks is, therefore, to align these labels.\n",
    "\n",
    "Label Alignment Strategies are as follow:\n",
    "\n",
    "1. **First Approach**: Keep the predicted labels static and process the real labels to facilitate the backward propagation process. This step is implemented in `From data_utils import DismatchIndexPadRawData`. The primary idea is to compare the two sets of labels one by one, identifying the index corresponding to the actual label. If the actual label is greater than the predicted label, the index corresponding to the real label is deleted. Conversely, if the actual label is smaller, the value of the predicted label is inserted at the index of the actual label. The type of transmembrane protein at that position is inferred based on the indices immediately preceding and following the true label.\n",
    "\n",
    "2. **Second Approach**: Maintain the actual labels unchanged and process the predicted labels to achieve atomic-level accuracy. Following the indices obtained from the first approach, if the actual label is larger than the predicted label, the corresponding value is inserted into the predicted label. If the actual label is smaller, the corresponding value in the predicted label is removed.\n",
    "\n",
    "3. **Third Approach**: Based on the second strategy, the predicted labels are segmented according to the length of atoms within each amino acid (proteins are composed of amino acids, which in turn consist of multiple atoms). After segmentation, a majority voting technique is used to determine the category of each atom within the predicted amino acid. The transmembrane category of the amino acid is decided by the majority vote, achieving residue-level accuracy (amino acid level). This part of the process is covered in `from task import MapAtomNode`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import CreateDataBeforeBatch,TMPDataset,CreateLable,MapAtomNode,node_accuracy,GaussianSmoothing,batchdata\n",
    "from data_utils import ProcessRawData,ParseStructure\n",
    "from torch.utils.data import DataLoader\n",
    "from test import TMPTest\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from egnnmodel import EGNNModel\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training and Model Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"image/egnn_atom.png\" width=\"400\" height=\"280\" alt=\"GCPNet\">\n",
    "  <img src=\"image/egnn_residual.png\" width=\"400\" height=\"280\" alt=\"Second Image\">\n",
    "  <img src=\"image/egnn_loss.png\" width=\"400\" height=\"280\" alt=\"Third Image\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Atom-Level Accuracy:**\n",
    "\n",
    "    - The training accuracy at the atom level remains consistently high across all epochs, as indicated by the flat line in the first graph.\n",
    "    - The validation accuracy at the atom level also shows stability\n",
    "    - The closeness of the training and validation accuracy lines suggests that the model generalizes well, with no apparent overfitting or underfitting.\n",
    "2. **Residual-Level Accuracy:**\n",
    "\n",
    "    - Similar to atom-level accuracy, the training accuracy at the residue level is high and stable.\n",
    "    - The validation accuracy is also quite stable, and the lines for training and validation accuracy nearly overlap, indicating good generalization at the residue level too.\n",
    "\n",
    "3. **Loss Over Epochs:**\n",
    "\n",
    "    - The training loss drops sharply in the initial epochs and then levels off, indicating that the model quickly learns patterns from the training data.\n",
    "    - The validation loss decreases along with the training loss but also stabilizes, suggesting that there are no significant improvements in the model after a certain point.\n",
    "    - The proximity of training and validation loss indicates that the model is not overfitting on the training data and performs similarly on unseen data.\n",
    "\n",
    "    In summary, this model demonstrates stable accuracy in both training and validation at the atom and residue levels, with no significant signs of overfitting. The loss metrics confirm this, showing a good fit of the model after the initial learning phase without further improvement. The performance at both the atom and residue levels suggests that the model has generalized well from the training to the validation data.\n",
    "\n",
    "    Further assessment is required to test the model and draw conclusions, which will be presented in the following section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization(split data to setup1-5/download pdb/parse pdb)\n",
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "processor = ProcessRawData(path,file_name)\n",
    "processor.run() # split data and download the pdb\n",
    "processor = ParseStructure(path)\n",
    "processor.run() # prase pdb and store them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "batch_size=100\n",
    "setup = 'setup1' # choose crossvalidation (total 5)\n",
    "processsor= CreateDataBeforeBatch(path)\n",
    "train_data_dict_before_batch,val_data_dict_before_batch,test_data_dict_before_batch=processsor.get_data(setup)\n",
    "\n",
    "## dataloader for processing label \n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put train label togther to increase the utilization of GPU\n",
    "train_residual_level_label={}\n",
    "train_atom_levl_label = {}\n",
    "train_dismatch_index_pred ={}\n",
    "train_dismatch_index_type ={}\n",
    "for data_batch in train_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,df_train,_,_=labelprocessor.labeldispatcher(setup,subset='train')\n",
    "    train_atom_levl_label.update(atom_level_label_dict) \n",
    "    train_residual_level_label.update(redidual_level_label_dict) \n",
    "    train_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    train_dismatch_index_type.update(dismatch_index_type)\n",
    "# put val label togther\n",
    "val_residual_level_label={}\n",
    "val_atom_levl_label = {}\n",
    "val_dismatch_index_pred ={}\n",
    "val_dismatch_index_type ={}\n",
    "for data_batch in val_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,_,df_val,_=labelprocessor.labeldispatcher(setup,subset='val')\n",
    "    val_atom_levl_label.update(atom_level_label_dict) \n",
    "    val_residual_level_label.update(redidual_level_label_dict) \n",
    "    val_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    val_dismatch_index_type.update(dismatch_index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for model \n",
    "batch_size=1\n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model \n",
    "max_len= 20000*batch_size\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = EGNNModel(out_dim=6,max_len=max_len,num_layers=5,emb_dim=128,residual=True,dropout=0.1).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=50\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_residual_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_residual_level_accuracy_record_val = []\n",
    "smoothing = GaussianSmoothing(6, 29, 5)\n",
    "for epoch in range(total_epochs):\n",
    "     epoch_atom_level_accuracy_train = []\n",
    "     epoch_loss_train=[]\n",
    "     epoch_residual_level_accuracy_train = []\n",
    "     # train\n",
    "     for data_batch in train_data_loader:\n",
    "          global_step += 1 \n",
    "          batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "          label_part = [value.unsqueeze(0) for name in batchname for value in train_atom_levl_label[name].to_dense()]\n",
    "          atom_levl_label = torch.cat(label_part).to(device)\n",
    "          residual_level_label = [value for name in batchname for value in train_residual_level_label[name]]\n",
    "          data =batchdata(data_batch) \n",
    "          optimizer.zero_grad()  \n",
    "          outputs = model(data.to(device)) \n",
    "          prediction = outputs[\"node_embedding\"] \n",
    "          predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "          predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "          predicted = smoothing(predicted)\n",
    "          prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "          loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "          loss.backward()\n",
    "          optimizer.step() \n",
    "\n",
    "          #calulate atom-level accuracy and node-level accuracy\n",
    "          _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "          correct = (predicted == atom_levl_label ).sum().item()\n",
    "          total = atom_levl_label.size(0)\n",
    "          atom_level_accuracy =  correct / total\n",
    "\n",
    "          processor = MapAtomNode(predicted.cpu(),batchname,train_dismatch_index_pred,train_dismatch_index_type,df_train)\n",
    "          train_predict_node_label = processor.map_atom_node() \n",
    "          residual_level_accuracy = node_accuracy(train_predict_node_label,residual_level_label)\n",
    "          epoch_loss_train.append(loss.item())\n",
    "          epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "          epoch_residual_level_accuracy_train.append(residual_level_accuracy)\n",
    "     epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "     epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "     epoch_residual_level_accuracy_record_train.append(np.mean(epoch_residual_level_accuracy_train))\n",
    "    \n",
    "\n",
    "     # val\n",
    "     model.eval()  \n",
    "     with torch.no_grad():  \n",
    "          epoch_atom_level_accuracy_val = []\n",
    "          epoch_loss_val = []\n",
    "          epoch_residual_level_accuracy_val = []\n",
    "          for data_batch in val_data_loader:\n",
    "               batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "               label_part = [value.unsqueeze(0) for name in batchname for value in val_atom_levl_label[name].to_dense()]\n",
    "               atom_levl_label = torch.cat(label_part).to(device)\n",
    "               residual_level_label = [value for name in batchname for value in val_residual_level_label[name]]\n",
    "               data =batchdata(data_batch)\n",
    "               outputs = model(data.to(device)) \n",
    "               prediction = outputs[\"node_embedding\"] \n",
    "               predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "               predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "               predicted = smoothing(predicted)\n",
    "               prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "               loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "               _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "               correct = (predicted == atom_levl_label ).sum().item()\n",
    "               total = atom_levl_label.size(0)\n",
    "               atom_level_accuracy =  correct / total\n",
    "               processor = MapAtomNode(predicted.cpu(),batchname,val_dismatch_index_pred,val_dismatch_index_type,df_val)\n",
    "               val_predict_node_label = processor.map_atom_node() \n",
    "               residual_level_accuracy = node_accuracy(val_predict_node_label,residual_level_label)\n",
    "               epoch_loss_val.append(loss.item())\n",
    "               epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "               epoch_residual_level_accuracy_val.append(residual_level_accuracy)\n",
    "          epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "          epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "          epoch_residual_level_accuracy_record_val.append(np.mean(epoch_residual_level_accuracy_val))\n",
    "print(\"Finished training.\")\n",
    "torch.save(model.state_dict(), '/work3/s230027/DL/result/egnn/egnn_model_size1_epoch50.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Test and Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five types porteins in the test data, including TM,SP_TM,BETA,Gloabl,Singal. But we foucs on the TM,SP_TM,BETA and want to the correct topology. We get correct topology is 0. There are some reasons:\n",
    "\n",
    "1. insufficient classification performance for specific protein types\n",
    "\n",
    "2. model structure issues:\n",
    "\n",
    "    - The model structure may be ill-suited for processing certain types of protein sequences, which could negatively impact the model's performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path = '/work3/s230027/DL/codebase/dataset/parse raw data/cv4.pickle'\n",
    "with open(path, 'rb') as file:\n",
    "    cv4 = pickle.load(file)\n",
    "\n",
    "TM_name_list = cv4[cv4['protein_type'] == 'TM']['uniprot_id_low'].tolist()\n",
    "BETA_name_list = cv4[cv4['protein_type'] == 'BETA']['uniprot_id_low'].tolist()\n",
    "SP_TM_name_list = cv4[cv4['protein_type'] == 'SP+TM']['uniprot_id_low'].tolist()\n",
    "\n",
    "\n",
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "batch_size=100\n",
    "setup = 'setup1' # choose crossvalidation (total 5)\n",
    "processsor= CreateDataBeforeBatch(path)\n",
    "train_data_dict_before_batch,val_data_dict_before_batch,test_data_dict_before_batch=processsor.get_data(setup)\n",
    "\n",
    "TM_test={}\n",
    "for name in TM_name_list:\n",
    "    TM_test[name]=test_data_dict_before_batch[name]\n",
    "\n",
    "BETA_test={}\n",
    "for name in BETA_name_list:\n",
    "    BETA_test[name]=test_data_dict_before_batch[name]\n",
    "\n",
    "SP_TM_test={}\n",
    "for name in SP_TM_name_list:\n",
    "    SP_TM_test[name] = test_data_dict_before_batch[name]\n",
    "\n",
    "\n",
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "modelpath = '/work3/s230027/DL/result/egnn/egnn_model_size1_epoch100.pth'\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TM\n",
    "processor=TMPTest(TM_test,file_name,path,batch_size,5,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()\n",
    "##SP_TM\n",
    "processor=TMPTest(SP_TM_test,file_name,path,batch_size,5,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()\n",
    "##BETA\n",
    "processor=TMPTest(BETA_test,file_name,path,batch_size,3,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **GCPNET Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Architecture**\n",
    "\n",
    "\n",
    "1. **Graph Definition**: The model begins by defining the input data as a graph using `torch_geometric`. This includes transforming raw data (the 3D structure of proteins) into a graph format where nodes represent atoms or residues and edges represent the connections or relationships between these nodes.\n",
    "\n",
    "2. **Geometry-Complete Graph Convolution with GCPNet**: The model employs a series of graph convolution processes (GCP) to update the features of nodes and edges.\n",
    "   - **Node Tensors \\(H\\)**: Node features are processed through multiple GCP layers, resulting in transformed node tensors.\n",
    "\n",
    "   - **Edge Tensors \\(E\\)**: Edge features are similarly updated through GCP layers.\n",
    "   \n",
    "   - **Frames \\(F\\)**: Additional geometric information might be processed through frame messages, utilizing the relative positions or orientations of atoms in 3D space.\n",
    "\n",
    "3. **GCPNet Convolution**: The updated node and edge tensors are passed through GCPNet convolutional layers. Features and geometric information are merged to refine the graph representation further.\n",
    "\n",
    "4. **Output Graph \\(g^L\\)**: The output is a graph transformed by GCPNet, encoding features and structural information suitable for downstream tasks.\n",
    "\n",
    "\n",
    "\n",
    "|                    |\n",
    "|:-------------------:|\n",
    "| <p align=\"center\"><img src=\"image/gcpnet.png\" alt=\"Image\" width=\"500\" height=\"250\"></p> [8]|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Parameter**\n",
    "\n",
    "| Parameter                | Value   | Description                          |\n",
    "|--------------------------|---------|--------------------------------------|\n",
    "| `num_layers`             | 4       | Number of layers in the model        |\n",
    "| `emb_dim`                | 64      | Embedding dimension                  |\n",
    "| `node_s_emb_dim`         | 64      | Node scalar embedding dimension      |\n",
    "| `node_v_emb_dim`         | 8       | Node vector embedding dimension      |\n",
    "| `edge_s_emb_dim`         | 16      | Edge scalar embedding dimension      |\n",
    "| `edge_v_emb_dim`         | 2       | Edge vector embedding dimension      |\n",
    "| `r_max`                  | 10.0    | Maximum radius for interactions      |\n",
    "| `num_rbf`                | 8       | Number of radial basis functions     |\n",
    "| `activation`             | 'silu'  | Activation function                  |\n",
    "| `pool`                   | 'sum'   | Pooling method                       |\n",
    "| `learning rate`          | 0.01    | Learning rate for training           |\n",
    "| `weight_decay`           | 1e-4    | Weight decay for regularization      |\n",
    "| `batch size  `           | 1   |batch size      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paramters \n",
    "num_layers = 4\n",
    "emb_dim = 64\n",
    "node_s_emb_dim = emb_dim\n",
    "node_v_emb_dim = 8\n",
    "edge_s_emb_dim = 16\n",
    "edge_v_emb_dim = 2\n",
    "r_max = 10.0\n",
    "num_rbf = 8\n",
    "activation = 'silu'\n",
    "pool = 'sum'\n",
    "module_cfg = OmegaConf.create({\n",
    "    'norm_pos_diff': True,\n",
    "    'scalar_gate': 0,\n",
    "    'vector_gate': True,\n",
    "    'scalar_nonlinearity': activation,\n",
    "    'vector_nonlinearity': activation,\n",
    "    'nonlinearities': [activation, activation],\n",
    "    'r_max': r_max,\n",
    "    'num_rbf': num_rbf,\n",
    "    'bottleneck': 2,\n",
    "    'vector_linear': True,\n",
    "    'vector_identity': True,\n",
    "    'default_bottleneck': 2,\n",
    "    'predict_node_positions': True,\n",
    "    'predict_node_rep': True,\n",
    "    'node_positions_weight': 1.0,\n",
    "    'update_positions_with_vector_sum': False,\n",
    "    'enable_e3_equivariance': False,\n",
    "    'pool': pool,\n",
    "})\n",
    "# model_cfg \n",
    "model_cfg = OmegaConf.create({\n",
    "    'h_input_dim': 1,  \n",
    "    'chi_input_dim': 2,     \n",
    "    'e_input_dim': 9, \n",
    "    'xi_input_dim': 1, \n",
    "    'h_hidden_dim': node_s_emb_dim,\n",
    "    'chi_hidden_dim': node_v_emb_dim,\n",
    "    'e_hidden_dim': edge_s_emb_dim,\n",
    "    'xi_hidden_dim': edge_v_emb_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': 0.0,\n",
    "})\n",
    "\n",
    "# layer_cfg \n",
    "layer_cfg = OmegaConf.create({\n",
    "    'pre_norm': False,\n",
    "    'use_gcp_norm': True,\n",
    "    'use_gcp_dropout': True,\n",
    "    'use_scalar_message_attention': True,\n",
    "    'num_feedforward_layers': 2,\n",
    "    'dropout': 0.0,\n",
    "    'nonlinearity_slope': 1e-2,\n",
    "    'mp_cfg': {\n",
    "        'edge_encoder': False,\n",
    "        'edge_gate': False,\n",
    "        'num_message_layers': 4,\n",
    "        'message_residual': 0,\n",
    "        'message_ff_multiplier': 1,\n",
    "        'self_message': True,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Downstream Tasks**\n",
    "\n",
    "The downstream tasks of GCPNET model are the same as EGNN model \n",
    "\n",
    "For our classification task predicting transmembrane proteins, we have a total of six categories. Thus, we have added two fully connected layers after the GCPNET model. The activation function employed is ReLU, and the output size is set to 6, corresponding to the number of classification categories. Furthermore, we have applied Gaussian Smoothing to the output (the `GaussianSmoothing` part is included in `from task import GaussianSmoothing`).\n",
    "\n",
    "##### Label Alignment for Downstream Tasks\n",
    "\n",
    "Given that the model's input originates from the Alphafold model developed by DeepMind—which cannot predict protein structures with complete accuracy—there are disparities between the model's predicted labels (Alphafold's entire sequence of predicted atoms) and the original labels (from the actual protein atomic sequences). One of the downstream tasks is, therefore, to align these labels.\n",
    "\n",
    "Label Alignment Strategies as follow:\n",
    "\n",
    "1. **First Approach**: Keep the predicted labels static and process the real labels to facilitate the backward propagation process. This step is implemented in `From data_utils import DismatchIndexPadRawData`. The primary idea is to compare the two sets of labels one by one, identifying the index corresponding to the actual label. If the actual label is greater than the predicted label, the index corresponding to the real label is deleted. Conversely, if the actual label is smaller, the value of the predicted label is inserted at the index of the actual label. The type of transmembrane protein at that position is inferred based on the indices immediately preceding and following the true label.\n",
    "\n",
    "2. **Second Approach**: Maintain the actual labels unchanged and process the predicted labels to achieve atomic-level accuracy. Following the indices obtained from the first approach, if the actual label is larger than the predicted label, the corresponding value is inserted into the predicted label. If the actual label is smaller, the corresponding value in the predicted label is removed.\n",
    "\n",
    "3. **Third Approach**: Based on the second strategy, the predicted labels are segmented according to the length of atoms within each amino acid (proteins are composed of amino acids, which in turn consist of multiple atoms). After segmentation, a majority voting technique is used to determine the category of each atom within the predicted amino acid. The transmembrane category of the amino acid is decided by the majority vote, achieving residue-level accuracy (amino acid level). This part of the process is covered in `from task import MapAtomNode`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import CreateDataBeforeBatch,TMPDataset,CreateLable,MapAtomNode,node_accuracy,ProcessBatch,GaussianSmoothing\n",
    "from data_utils import ProcessRawData,ParseStructure\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from gcpnet import GCPNetModel\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Training and Model Validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"image/gcpnet_atom.png\" width=\"400\" height=\"280\" alt=\"GCPNet\">\n",
    "  <img src=\"image/gcpnet_residual.png\" width=\"400\" height=\"280\" alt=\"Second Image\">\n",
    "  <img src=\"image/gcpnet_loss.png\" width=\"400\" height=\"280\" alt=\"Third Image\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Atom-Level Accuracy:**\n",
    "\n",
    "    - Throughout the training process, the training accuracy at the atom level remains extremely stable with almost no fluctuation, indicating that the model consistently maintains high accuracy on the training set data.\n",
    "    - The accuracy on the validation set is equally stable and very close to the training accuracy curve, suggesting that the model performs similarly on unseen data, demonstrating good generalization capability.\n",
    "    - The closeness of the training and validation accuracy curves indicates that there are no signs of overfitting or underfitting at the atom level, which is ideal in machine learning models.\n",
    "    - The accuracy of the model is slightly above 60.6%, and it remains consistent after about 50k steps, indicating that further training does not significantly improve accuracy.\n",
    "\n",
    "2. **Residue-Level Accuracy:**\n",
    "\n",
    "    - Similar to the atom level, the training accuracy at the residue level also shows a high degree of stability, suggesting that the model learns well from the data at the residue level.\n",
    "    - The validation accuracy almost overlaps with the training accuracy, indicating the model also generalizes well at the residue level, with good predictive performance on unknown data.\n",
    "    - The consistency of the two curves further proves the robustness and reliability of the model at the residue level.\n",
    "    - The model's accuracy is slightly above 60.7%, and it remains consistent after about 50k steps, indicating that further training does not significantly improve accuracy.\n",
    "\n",
    "3. **Loss Over Epochs:**\n",
    "\n",
    "    - The training loss drops sharply in the initial epochs and then levels off, signifying rapid progress in the initial stages of model learning. The stable trend of the training loss indicates that the model quickly reaches a point of minimized loss and does not change significantly in subsequent training.\n",
    "    - The validation loss  indicates that the model performs similarly on the validation set as it does on the training set, with low and stable loss suggesting good predictive ability on new data.\n",
    "    - The close proximity of the training and validation loss curves implies that the model is not overfitting on the training data and generalizes well to unseen data.\n",
    "\n",
    "    In summary, the GCPNet model demonstrates good stability and generalization ability at both the atom and residue levels. The loss curves indicate that the model learns quickly in the early stages of training and maintains that performance throughout. High accuracy at both the atom and residue levels indicates that the model learns well from the training data and has a good predictive ability on new data.\n",
    "\n",
    "    Further assessment is required to test the model and draw conclusions, which will be presented in the following section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCPNetModel(\n",
    "    num_layers=num_layers,\n",
    "    node_s_emb_dim=node_s_emb_dim,\n",
    "    node_v_emb_dim=node_v_emb_dim,\n",
    "    edge_s_emb_dim=edge_s_emb_dim,\n",
    "    edge_v_emb_dim=edge_v_emb_dim,\n",
    "    r_max=r_max,\n",
    "    num_rbf=num_rbf,\n",
    "    activation=activation,\n",
    "    pool=pool,\n",
    "    module_cfg=module_cfg,\n",
    "    model_cfg=model_cfg,\n",
    "    layer_cfg=layer_cfg\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=20\n",
    "draw_num = 1\n",
    "global_step = 0\n",
    "\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_residual_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_residual_level_accuracy_record_val = []\n",
    "\n",
    "smoothing = GaussianSmoothing(6, 29, 5)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "     epoch_atom_level_accuracy_train = []\n",
    "     epoch_loss_train=[]\n",
    "     epoch_residual_level_accuracy_train = []\n",
    "     # train\n",
    "     for data_batch in train_data_loader:\n",
    "          global_step += 1 \n",
    "          batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "          label_part = [value.unsqueeze(0) for name in batchname for value in train_atom_levl_label[name].to_dense()]\n",
    "          atom_levl_label = torch.cat(label_part).to(device)\n",
    "          residual_level_label = [value for name in batchname for value in train_residual_level_label[name]]\n",
    "                 \n",
    "          batchprocessor = ProcessBatch()\n",
    "          data = batchprocessor.batchdata(data_batch) \n",
    "          optimizer.zero_grad()  \n",
    "          outputs = model(data.to(device)) \n",
    "          prediction = outputs[\"node_embedding\"] \n",
    "\n",
    "          predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "          predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "          predicted = smoothing(predicted)\n",
    "          prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "          loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "          loss.backward()\n",
    "          optimizer.step() \n",
    "\n",
    "          #calulate atom-level accuracy and node-level accuracy\n",
    "          _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "          correct = (predicted == atom_levl_label).sum().item()\n",
    "          total = atom_levl_label.size(0)\n",
    "          atom_level_accuracy =  correct / total\n",
    "\n",
    "          processor = MapAtomNode(predicted.cpu(),batchname,train_dismatch_index_pred,train_dismatch_index_type,df_train)\n",
    "          train_predict_node_label = processor.map_atom_node() \n",
    "          residual_level_accuracy = node_accuracy(train_predict_node_label,residual_level_label)\n",
    "     \n",
    "          epoch_loss_train.append(loss.item())\n",
    "          epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "          epoch_residual_level_accuracy_train.append(residual_level_accuracy)\n",
    "\n",
    "     epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "     epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "     epoch_residual_level_accuracy_record_train.append(np.mean(epoch_residual_level_accuracy_train))\n",
    "\n",
    "     # val\n",
    "     model.eval()  \n",
    "     with torch.no_grad():  \n",
    "\n",
    "          epoch_atom_level_accuracy_val = []\n",
    "          epoch_loss_val = []\n",
    "          epoch_residual_level_accuracy_val = []\n",
    "\n",
    "          for data_batch in val_data_loader:\n",
    "\n",
    "               batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "               label_part = [value.unsqueeze(0) for name in batchname for value in val_atom_levl_label[name].to_dense()]\n",
    "               atom_levl_label = torch.cat(label_part).to(device)\n",
    "               residual_level_label = [value for name in batchname for value in val_residual_level_label[name]]\n",
    "               batchprocessor = ProcessBatch()\n",
    "               data = batchprocessor.batchdata(data_batch) \n",
    "\n",
    "               outputs = model(data.to(device)) \n",
    "               prediction = outputs[\"node_embedding\"] \n",
    "\n",
    "               predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "               predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "               predicted = smoothing(predicted)\n",
    "               prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "\n",
    "               loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "\n",
    "               _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "               correct = (predicted == atom_levl_label ).sum().item()\n",
    "               total = atom_levl_label.size(0)\n",
    "               atom_level_accuracy =  correct / total\n",
    "\n",
    "\n",
    "               processor = MapAtomNode(predicted.cpu(),batchname,val_dismatch_index_pred,val_dismatch_index_type,df_val)\n",
    "               val_predict_node_label = processor.map_atom_node() \n",
    "               residual_level_accuracy = node_accuracy(val_predict_node_label,residual_level_label)\n",
    "\n",
    "               epoch_loss_val.append(loss.item())\n",
    "               epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "               epoch_residual_level_accuracy_val.append(residual_level_accuracy)\n",
    "\n",
    "          epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "          epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "          epoch_residual_level_accuracy_record_val.append(np.mean(epoch_residual_level_accuracy_val))\n",
    "\n",
    "print(\"Finished training.\")\n",
    "\n",
    "node_acc_results = np.concatenate([ [np.array(epoch_residual_level_accuracy_record_train)], [np.array(epoch_residual_level_accuracy_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/gcpnet/CVsetup1_residual_acc_results.csv\", node_acc_results, delimiter=',', comments=\"\", fmt='%s')\n",
    "loss_results = np.concatenate([[np.array(epoch_loss_record_train)], [np.array(epoch_loss_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/gcpnet/CVsetup1_loss_results.csv\", loss_results, delimiter=',', comments=\"\", fmt='%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Model Test and Result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. **Performance on TM, SP_TM, BETA Proteins:**\n",
    "   - The model achieved a correct topology score of 0 for TM, SP_TM, and BETA proteins.This suggests a significant limitation in the model's ability to predict the topology for these specific protein types. It may indicate that the model has not learned the necessary features to distinguish between the correct topologies of these proteins or that the current model architecture and training are not aligned with the complexity of these protein structures.\n",
    "\n",
    "2. **Implications for Model Improvement:**\n",
    "   - The model's current performance highlights the existence of substantial room for improvement, particularly for TM, SP_TM, and BETA proteins. Potential areas of improvement may include revisiting the feature extraction process to ensure that crucial information for topology prediction is not lost.\n",
    "   - Another avenue for improvement could involve incorporating additional data or utilizing data augmentation techniques to enhance the model's ability to generalize from training to unseen data.\n",
    "   - Consideration of more complex or specialized model architectures may also be beneficial in capturing the intricacies of protein structure prediction.\n",
    "\n",
    "3. **Others**\n",
    "   - The test accuarcy data from the EGNN are the same as from the GCPNET model, the reason we infer is adding the same Gaussian smoothing in two model to deal with the outputs from models.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "modelpath='/work3/s230027/DL/result/gcpnet/CVsetup1_model_major_voting_size1_epoch20.pth'\n",
    "batch_size=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TM\n",
    "processor=TMPTest(TM_test,file_name,path,batch_size,5,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()\n",
    "##SP_TM\n",
    "processor=TMPTest(SP_TM_test,file_name,path,batch_size,5,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()\n",
    "##BETA\n",
    "processor=TMPTest(BETA_test,file_name,path,batch_size,3,setup='setup1',modelpath=modelpath)\n",
    "processor.printresult()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Discussion**\n",
    "\n",
    "The comparison between the trained SchNet models and the baseline model shows that SchNet models are better at determining the residue-wise topology classes. Nonetheless, the overall topological prediction is far from satisfactory. \n",
    "\n",
    "One of the main reasons for this could be that the model was trained from scratch without using any pre-trained weight. The current state-of-the-art models like DeepTMHMM and TMbed are based on sophisticated protein language models (pLMs), which are pre-trained on billions of protein sequences. \n",
    "\n",
    "Furthermore, the time and computational constraints prevented us from experimenting with other GNNs, which might be more well-suited for the topology task. Finally, the amounts of the protein types alpha TM,  alpha SP+TM and  beta barrel are way less compared to  Globular and SP+ Globular as shown in the part of Protein Types and 3D Predictions Availaility.\n",
    "\n",
    "This data imbalance most likely has contributed to the fact that the model failed to predict the topologies for these three particular protein types. Finally, it is important to emphasize that the 3D structures predicted by Alphafold also contain uncertainties, which inevitably would limit the performance of the models that are trained upon AlphaDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Conclusion**\n",
    "\n",
    "In this study, we have trained state-of-the-art GNNs such as SchNet to perform topological tasks. We devised the major voting method to infer a residue-wise topology by gathering the atom level topological predictions. The performance of the final SchNet models is still far away from competing with the protein models such as DeepTMHMM and TMbed. Nonetheless, with more pre-trained GNNs becoming available with respect to the topological task, the performance is certainly to be improved in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "[1]Bruce Alberts, Dennis Bray, Karen Hopkin, Alexan-\n",
    "der D Johnson, Julian Lewis, Martin Raff, Keith\n",
    "Roberts, and Peter Walter, Essential cell biology, Gar-\n",
    "land Science, 2015.\n",
    "\n",
    "[2] Andreas Bernsel, H ̊akan Viklund, Aron Hennerdal, and\n",
    "Arne Elofsson, “Topcons: consensus prediction of\n",
    "membrane protein topology,” Nucleic acids research,\n",
    "vol. 37, no. suppl 2, pp. W465–W468, 2009.\n",
    "\n",
    "[3] L ́aszl ́o Dobson, Istv ́an Rem ́enyi, and G ́abor E Tusn ́ady,\n",
    "“Cctop: a consensus constrained topology prediction\n",
    "web server,” Nucleic acids research, vol. 43, no. W1,\n",
    "pp. W408–W412, 2015.\n",
    "\n",
    "[4] John Jumper, Richard Evans, Alexander Pritzel, Tim\n",
    "Green, Michael Figurnov, Olaf Ronneberger, Kathryn\n",
    "Tunyasuvunakool, Russ Bates, Augustin ˇZ ́ıdek, Anna\n",
    "Potapenko, et al., “Highly accurate protein structure pre-\n",
    "diction with alphafold,” Nature, vol. 596, no. 7873, pp.\n",
    "583–589, 2021.\n",
    "\n",
    "[5] Tue Herlau, Mikkel N Schmidt, and Morten Mørup, “In-\n",
    "troduction to machine learning and data mining,” Lec-\n",
    "ture notes of the course of the same name given at DTU\n",
    "(Technical University of Denmark), 2022.\n",
    "\n",
    "\n",
    "[6] Schütt, K. T., Sauceda, H. E., Kindermans, P.-J., Tkatchenko, A., & Müller, K.-R. (2018). SchNet – A deep learning architecture for molecules and materials. Journal of Chemical Physics, 148, 241722. https://doi.org/10.1063/1.5019779\n",
    "\n",
    "\n",
    "\n",
    "[7] Anonymous, “Evaluating representation learning on the\n",
    "protein structure universe,” in Submitted to The Twelfth\n",
    "International Conference on Learning Representations,\n",
    "2023, under review.\n",
    "\n",
    "[8]Airas, J., Ding, X., & Zhang, B. (2023). Transferable Implicit Solvation via Contrastive Learning of Graph Neural Networks. \n",
    "ACS Central Science. Advance online publication. https://doi.org/10.1021/acscentsci.3c01160\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
