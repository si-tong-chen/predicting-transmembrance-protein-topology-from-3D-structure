{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import CreateDataBeforeBatch,TMPDataset,CreateLable,MapAtomNode,node_accuracy,GaussianSmoothing,batchdata\n",
    "from data_utils import ProcessRawData,ParseStructure\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from egnnmodel import EGNNModel\n",
    "import numpy as np\n",
    "import wandb\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DL_egnn_new\", #项目名称\n",
    "    entity=\"transmembrane-topology\", # 用户名\n",
    "    group=\"CV setup1\", # 对比实验分组\n",
    "    name= \"epoch50_size1 \", #实验的名字\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"egnn\",\n",
    "    \"dataset\": \"protein 3D structures \",\n",
    "    \"epochs\":50,\n",
    "    'batch_size':1,\n",
    "    'hidden_channels' :128,\n",
    "    'weight_decay': 1e-4\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization(split data to setup1-5/download pdb/parse pdb)\n",
    "\n",
    "# file_name = \"DeepTMHMM.3line\"\n",
    "# path='/work3/s230027/DL/codebase/'\n",
    "# processor = ProcessRawData(path,file_name)\n",
    "# processor.run() # split data and download the pdb\n",
    "\n",
    "# processor = ParseStructure(path)\n",
    "# processor.run() # prase pdb and store them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "batch_size=100\n",
    "setup = 'setup1' # choose crossvalidation (total 5)\n",
    "processsor= CreateDataBeforeBatch(path)\n",
    "train_data_dict_before_batch,val_data_dict_before_batch,test_data_dict_before_batch=processsor.get_data(setup)\n",
    "\n",
    "## dataloader for processing label \n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put train label togther\n",
    "train_residual_level_label={}\n",
    "train_atom_levl_label = {}\n",
    "train_dismatch_index_pred ={}\n",
    "train_dismatch_index_type ={}\n",
    "for data_batch in train_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,df_train,_,_=labelprocessor.labeldispatcher(setup,subset='train')\n",
    "\n",
    "    train_atom_levl_label.update(atom_level_label_dict) \n",
    "    train_residual_level_label.update(redidual_level_label_dict) \n",
    "    train_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    train_dismatch_index_type.update(dismatch_index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put val label togther\n",
    "val_residual_level_label={}\n",
    "val_atom_levl_label = {}\n",
    "val_dismatch_index_pred ={}\n",
    "val_dismatch_index_type ={}\n",
    "for data_batch in val_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,_,df_val,_=labelprocessor.labeldispatcher(setup,subset='val')\n",
    "    val_atom_levl_label.update(atom_level_label_dict) \n",
    "    val_residual_level_label.update(redidual_level_label_dict) \n",
    "    val_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    val_dismatch_index_type.update(dismatch_index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for model \n",
    "batch_size=1\n",
    "\n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len= 20000*batch_size\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = EGNNModel(out_dim=6,max_len=max_len,num_layers=5,emb_dim=128,residual=True,dropout=0.1).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=50\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_residual_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_residual_level_accuracy_record_val = []\n",
    "smoothing = GaussianSmoothing(6, 29, 5)\n",
    "for epoch in range(total_epochs):\n",
    "     epoch_atom_level_accuracy_train = []\n",
    "     epoch_loss_train=[]\n",
    "     epoch_residual_level_accuracy_train = []\n",
    "     # train\n",
    "     for data_batch in train_data_loader:\n",
    "          global_step += 1 \n",
    "          batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "\n",
    "          label_part = [value.unsqueeze(0) for name in batchname for value in train_atom_levl_label[name].to_dense()]\n",
    "          atom_levl_label = torch.cat(label_part).to(device)\n",
    "          residual_level_label = [value for name in batchname for value in train_residual_level_label[name]]\n",
    "\n",
    "          data =batchdata(data_batch) #按照batch_size进行组装\n",
    "\n",
    "          optimizer.zero_grad()  \n",
    "          outputs = model(data.to(device)) \n",
    "          prediction = outputs[\"node_embedding\"] \n",
    "          predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "          predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "          predicted = smoothing(predicted)\n",
    "          prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "          loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "          loss.backward()\n",
    "          optimizer.step() \n",
    "\n",
    "          #calulate atom-level accuracy and node-level accuracy\n",
    "          _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "          correct = (predicted == atom_levl_label ).sum().item()\n",
    "          total = atom_levl_label.size(0)\n",
    "          atom_level_accuracy =  correct / total\n",
    "\n",
    "          processor = MapAtomNode(predicted.cpu(),batchname,train_dismatch_index_pred,train_dismatch_index_type,df_train)\n",
    "          train_predict_node_label = processor.map_atom_node() \n",
    "          residual_level_accuracy = node_accuracy(train_predict_node_label,residual_level_label)\n",
    "\n",
    "          wandb.log({'train_loss_step':loss.item(), 'global_step':global_step})\n",
    "          wandb.log({'train_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "          wandb.log({'train_residual_level_accuracy_step':residual_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "\n",
    "          epoch_loss_train.append(loss.item())\n",
    "          epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "          epoch_residual_level_accuracy_train.append(residual_level_accuracy)\n",
    "\n",
    "     epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "     epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "     epoch_residual_level_accuracy_record_train.append(np.mean(epoch_residual_level_accuracy_train))\n",
    "     wandb.log({'train_loss_epoch':np.mean(epoch_loss_train), 'global_step':global_step})\n",
    "     wandb.log({'train_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_train),  'global_step':global_step})\n",
    "     wandb.log({'train_residual_level_accuracy_epoch':np.mean(epoch_residual_level_accuracy_train), 'global_step':global_step})\n",
    "    \n",
    "\n",
    "     # val\n",
    "     model.eval()  \n",
    "     with torch.no_grad():  \n",
    "\n",
    "          epoch_atom_level_accuracy_val = []\n",
    "          epoch_loss_val = []\n",
    "          epoch_residual_level_accuracy_val = []\n",
    "\n",
    "          for data_batch in val_data_loader:\n",
    "               batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "\n",
    "               label_part = [value.unsqueeze(0) for name in batchname for value in val_atom_levl_label[name].to_dense()]\n",
    "               atom_levl_label = torch.cat(label_part).to(device)\n",
    "               residual_level_label = [value for name in batchname for value in val_residual_level_label[name]]\n",
    "\n",
    "\n",
    "\n",
    "               data =batchdata(data_batch) #按照batch_size进行组装\n",
    "               outputs = model(data.to(device)) \n",
    "               prediction = outputs[\"node_embedding\"] \n",
    "\n",
    "               predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "               predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "               predicted = smoothing(predicted)\n",
    "               prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "               loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "\n",
    "               _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "               correct = (predicted == atom_levl_label ).sum().item()\n",
    "               total = atom_levl_label.size(0)\n",
    "               atom_level_accuracy =  correct / total\n",
    "\n",
    "               processor = MapAtomNode(predicted.cpu(),batchname,val_dismatch_index_pred,val_dismatch_index_type,df_val)\n",
    "               val_predict_node_label = processor.map_atom_node() \n",
    "               residual_level_accuracy = node_accuracy(val_predict_node_label,residual_level_label)\n",
    "\n",
    "               epoch_loss_val.append(loss.item())\n",
    "               epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "               epoch_residual_level_accuracy_val.append(residual_level_accuracy)\n",
    "\n",
    "               wandb.log({'val_loss_step':loss.item(), 'global_step':global_step})\n",
    "               wandb.log({'val_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "               wandb.log({'val_residual_level_accuracy_step':residual_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "\n",
    "          epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "          epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "          epoch_residual_level_accuracy_record_val.append(np.mean(epoch_residual_level_accuracy_val))\n",
    "          wandb.log({'val_loss_epoch':np.mean(epoch_loss_val), 'global_step':global_step})\n",
    "          wandb.log({'val_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_val), 'global_step':global_step})\n",
    "          wandb.log({'val_residual_level_accuracy_epoch':np.mean(epoch_residual_level_accuracy_val), 'global_step':global_step})\n",
    "\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "\n",
    "print(\"Finished training.\")\n",
    "\n",
    "torch.save(model.state_dict(), '/work3/s230027/DL/result/egnn/egnn_model_size1_epoch50.pth')\n",
    "print('epoch_residual_level_accuracy_record_train',epoch_residual_level_accuracy_record_train)\n",
    "print('epoch_residual_level_accuracy_record_val',epoch_residual_level_accuracy_record_val)\n",
    "print('epoch_loss_record_train',epoch_loss_record_train)\n",
    "print('epoch_loss_record_val',epoch_loss_record_val)\n",
    "\n",
    "node_acc_results = np.concatenate([ [np.array(epoch_residual_level_accuracy_record_train)], [np.array(epoch_residual_level_accuracy_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/egnn/CVsetup1_residual_acc_results.csv\", node_acc_results, delimiter=',', comments=\"\", fmt='%s')\n",
    "\n",
    "loss_results = np.concatenate([[np.array(epoch_loss_record_train)], [np.array(epoch_loss_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/egnn/CVsetup1_loss_results.csv\", loss_results, delimiter=',', comments=\"\", fmt='%s')\n",
    "\n",
    "\n",
    "   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
