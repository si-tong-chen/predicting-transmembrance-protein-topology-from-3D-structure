{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from task import CreateDataBeforeBatch,TMPDataset,CreateLable,MapAtomNode,node_accuracy,ProcessBatch,GaussianSmoothing\n",
    "from data_utils import ProcessRawData,ParseStructure\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn import functional as F\n",
    "from gcpnet import GCPNetModel\n",
    "from omegaconf import OmegaConf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization(split data to setup1-5/download pdb/parse pdb)\n",
    "\n",
    "# file_name = \"DeepTMHMM.3line\"\n",
    "# path='/work3/s230027/DL/codebase/'\n",
    "# processor = ProcessRawData(path,file_name)\n",
    "# processor.run() # split data and download the pdb\n",
    "\n",
    "# processor = ParseStructure(path)\n",
    "# processor.run() # prase pdb and store them \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"DeepTMHMM.3line\"\n",
    "path='/work3/s230027/DL/codebase/'\n",
    "batch_size=100\n",
    "setup = 'setup1' # choose crossvalidation (total 5)\n",
    "processsor= CreateDataBeforeBatch(path)\n",
    "train_data_dict_before_batch,val_data_dict_before_batch,test_data_dict_before_batch=processsor.get_data(setup)\n",
    "\n",
    "## dataloader for processing label \n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "test_dataset = TMPDataset(test_data_dict_before_batch)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put train label togther\n",
    "train_residual_level_label={}\n",
    "train_atom_levl_label = {}\n",
    "train_dismatch_index_pred ={}\n",
    "train_dismatch_index_type ={}\n",
    "for data_batch in train_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,df_train,_,_=labelprocessor.labeldispatcher(setup,subset='train')\n",
    "\n",
    "    train_atom_levl_label.update(atom_level_label_dict) \n",
    "    train_residual_level_label.update(redidual_level_label_dict) \n",
    "    train_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    train_dismatch_index_type.update(dismatch_index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put val label togther\n",
    "val_residual_level_label={}\n",
    "val_atom_levl_label = {}\n",
    "val_dismatch_index_pred ={}\n",
    "val_dismatch_index_type ={}\n",
    "for data_batch in val_data_loader:\n",
    "    batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "\n",
    "    labelprocessor=CreateLable(batchname,data_batch,path,file_name)\n",
    "    atom_level_label_dict,redidual_level_label_dict,dismatch_index_pred,dismatch_index_type,_,df_val,_=labelprocessor.labeldispatcher(setup,subset='val')\n",
    "    val_atom_levl_label.update(atom_level_label_dict) \n",
    "    val_residual_level_label.update(redidual_level_label_dict) \n",
    "    val_dismatch_index_pred.update(dismatch_index_pred)\n",
    "    val_dismatch_index_type.update(dismatch_index_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader for model \n",
    "batch_size=1\n",
    "\n",
    "train_dataset = TMPDataset(train_data_dict_before_batch)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)\n",
    "\n",
    "val_dataset = TMPDataset(val_data_dict_before_batch)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True,collate_fn=lambda x: x,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model paramters \n",
    "num_layers = 4\n",
    "emb_dim = 64\n",
    "node_s_emb_dim = emb_dim\n",
    "node_v_emb_dim = 8\n",
    "edge_s_emb_dim = 16\n",
    "edge_v_emb_dim = 2\n",
    "r_max = 10.0\n",
    "num_rbf = 8\n",
    "activation = 'silu'\n",
    "pool = 'sum'\n",
    "\n",
    "module_cfg = OmegaConf.create({\n",
    "    'norm_pos_diff': True,\n",
    "    'scalar_gate': 0,\n",
    "    'vector_gate': True,\n",
    "    'scalar_nonlinearity': activation,\n",
    "    'vector_nonlinearity': activation,\n",
    "    'nonlinearities': [activation, activation],\n",
    "    'r_max': r_max,\n",
    "    'num_rbf': num_rbf,\n",
    "    'bottleneck': 2,\n",
    "    'vector_linear': True,\n",
    "    'vector_identity': True,\n",
    "    'default_bottleneck': 2,\n",
    "    'predict_node_positions': True,\n",
    "    'predict_node_rep': True,\n",
    "    'node_positions_weight': 1.0,\n",
    "    'update_positions_with_vector_sum': False,\n",
    "    'enable_e3_equivariance': False,\n",
    "    'pool': pool,\n",
    "})\n",
    "\n",
    "# model_cfg \n",
    "model_cfg = OmegaConf.create({\n",
    "    'h_input_dim': 1,  \n",
    "    'chi_input_dim': 2,     \n",
    "    'e_input_dim': 9, \n",
    "    'xi_input_dim': 1, \n",
    "    'h_hidden_dim': node_s_emb_dim,\n",
    "    'chi_hidden_dim': node_v_emb_dim,\n",
    "    'e_hidden_dim': edge_s_emb_dim,\n",
    "    'xi_hidden_dim': edge_v_emb_dim,\n",
    "    'num_layers': num_layers,\n",
    "    'dropout': 0.0,\n",
    "})\n",
    "\n",
    "# layer_cfg \n",
    "layer_cfg = OmegaConf.create({\n",
    "    'pre_norm': False,\n",
    "    'use_gcp_norm': True,\n",
    "    'use_gcp_dropout': True,\n",
    "    'use_scalar_message_attention': True,\n",
    "    'num_feedforward_layers': 2,\n",
    "    'dropout': 0.0,\n",
    "    'nonlinearity_slope': 1e-2,\n",
    "    'mp_cfg': {\n",
    "        'edge_encoder': False,\n",
    "        'edge_gate': False,\n",
    "        'num_message_layers': 4,\n",
    "        'message_residual': 0,\n",
    "        'message_ff_multiplier': 1,\n",
    "        'self_message': True,\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"DL_gcpnet\", #项目名称\n",
    "    entity=\"transmembrane-topology\", # 用户名\n",
    "    group=\"CVsetup1\", # 对比实验分组\n",
    "    name= \"epoch20_size1 \", #实验的名字\n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"architecture\": \"gcpnet\",\n",
    "    \"dataset\": \"protein 3D structures \",\n",
    "    \"epochs\":20,\n",
    "    'batch_size':1,\n",
    "    'hidden_channels' :128,\n",
    "    }\n",
    ")\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = GCPNetModel(\n",
    "    num_layers=num_layers,\n",
    "    node_s_emb_dim=node_s_emb_dim,\n",
    "    node_v_emb_dim=node_v_emb_dim,\n",
    "    edge_s_emb_dim=edge_s_emb_dim,\n",
    "    edge_v_emb_dim=edge_v_emb_dim,\n",
    "    r_max=r_max,\n",
    "    num_rbf=num_rbf,\n",
    "    activation=activation,\n",
    "    pool=pool,\n",
    "    module_cfg=module_cfg,\n",
    "    model_cfg=model_cfg,\n",
    "    layer_cfg=layer_cfg\n",
    ").to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001,weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epochs=20\n",
    "draw_num = 1\n",
    "global_step = 0\n",
    "\n",
    "epoch_atom_level_accuracy_record_train = []\n",
    "epoch_loss_record_train=[]\n",
    "epoch_residual_level_accuracy_record_train = []\n",
    "epoch_atom_level_accuracy_record_val = []\n",
    "epoch_loss_record_val = []\n",
    "epoch_residual_level_accuracy_record_val = []\n",
    "\n",
    "smoothing = GaussianSmoothing(6, 29, 5)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "     epoch_atom_level_accuracy_train = []\n",
    "     epoch_loss_train=[]\n",
    "     epoch_residual_level_accuracy_train = []\n",
    "     # train\n",
    "     for data_batch in train_data_loader:\n",
    "          global_step += 1 \n",
    "          batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "          label_part = [value.unsqueeze(0) for name in batchname for value in train_atom_levl_label[name].to_dense()]\n",
    "          atom_levl_label = torch.cat(label_part).to(device)\n",
    "          residual_level_label = [value for name in batchname for value in train_residual_level_label[name]]\n",
    "          \n",
    "          \n",
    "          batchprocessor = ProcessBatch()\n",
    "          data = batchprocessor.batchdata(data_batch) \n",
    "          optimizer.zero_grad()  \n",
    "          outputs = model(data.to(device)) \n",
    "          prediction = outputs[\"node_embedding\"] \n",
    "\n",
    "          predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "          predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "          predicted = smoothing(predicted)\n",
    "          prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "          loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "          loss.backward()\n",
    "          optimizer.step() \n",
    "\n",
    "          #calulate atom-level accuracy and node-level accuracy\n",
    "          _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "          correct = (predicted == atom_levl_label).sum().item()\n",
    "          total = atom_levl_label.size(0)\n",
    "          atom_level_accuracy =  correct / total\n",
    "\n",
    "          processor = MapAtomNode(predicted.cpu(),batchname,train_dismatch_index_pred,train_dismatch_index_type,df_train)\n",
    "          train_predict_node_label = processor.map_atom_node() \n",
    "          residual_level_accuracy = node_accuracy(train_predict_node_label,residual_level_label)\n",
    "     \n",
    "\n",
    "          wandb.log({'train_loss_step':loss.item(), 'global_step':global_step})\n",
    "          wandb.log({'train_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "          wandb.log({'train_residual_level_accuracy_step':residual_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "\n",
    "\n",
    "          epoch_loss_train.append(loss.item())\n",
    "          epoch_atom_level_accuracy_train.append(atom_level_accuracy)\n",
    "          epoch_residual_level_accuracy_train.append(residual_level_accuracy)\n",
    "\n",
    "     epoch_loss_record_train.append(np.mean(epoch_loss_train))\n",
    "     epoch_atom_level_accuracy_record_train.append(np.mean(epoch_atom_level_accuracy_train))\n",
    "     epoch_residual_level_accuracy_record_train.append(np.mean(epoch_residual_level_accuracy_train))\n",
    "\n",
    "\n",
    "     wandb.log({'train_loss_epoch':np.mean(epoch_loss_train), 'global_step':global_step})\n",
    "     wandb.log({'train_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_train),  'global_step':global_step})\n",
    "     wandb.log({'train_residual_level_accuracy_epoch':np.mean(epoch_residual_level_accuracy_train), 'global_step':global_step})\n",
    "    \n",
    "\n",
    "     # val\n",
    "     model.eval()  \n",
    "     with torch.no_grad():  \n",
    "\n",
    "          epoch_atom_level_accuracy_val = []\n",
    "          epoch_loss_val = []\n",
    "          epoch_residual_level_accuracy_val = []\n",
    "\n",
    "          for data_batch in val_data_loader:\n",
    "\n",
    "               batchname=[data_batch[num]['name'] for num in range(len(data_batch))]\n",
    "               label_part = [value.unsqueeze(0) for name in batchname for value in val_atom_levl_label[name].to_dense()]\n",
    "               atom_levl_label = torch.cat(label_part).to(device)\n",
    "               residual_level_label = [value for name in batchname for value in val_residual_level_label[name]]\n",
    "               batchprocessor = ProcessBatch()\n",
    "               data = batchprocessor.batchdata(data_batch) \n",
    "\n",
    "               outputs = model(data.to(device)) \n",
    "               prediction = outputs[\"node_embedding\"] \n",
    "\n",
    "               predicted = torch.reshape(prediction.to('cpu'), (1,prediction.shape[1], prediction.shape[0]))\n",
    "               predicted = F.pad(predicted, (14, 14), mode='reflect')\n",
    "               predicted = smoothing(predicted)\n",
    "               prediction_Gauss = torch.reshape(predicted, (prediction.shape[0], prediction.shape[1]))\n",
    "\n",
    "               loss = criterion(prediction_Gauss.to(device), atom_levl_label)\n",
    "\n",
    "               _, predicted = torch.max(prediction_Gauss.to(device), 1) \n",
    "               correct = (predicted == atom_levl_label ).sum().item()\n",
    "               total = atom_levl_label.size(0)\n",
    "               atom_level_accuracy =  correct / total\n",
    "\n",
    "\n",
    "               processor = MapAtomNode(predicted.cpu(),batchname,val_dismatch_index_pred,val_dismatch_index_type,df_val)\n",
    "               val_predict_node_label = processor.map_atom_node() \n",
    "               residual_level_accuracy = node_accuracy(val_predict_node_label,residual_level_label)\n",
    "\n",
    "               epoch_loss_val.append(loss.item())\n",
    "               epoch_atom_level_accuracy_val.append(atom_level_accuracy)\n",
    "               epoch_residual_level_accuracy_val.append(residual_level_accuracy)\n",
    "\n",
    "               wandb.log({'val_loss_step':loss.item(), 'global_step':global_step})\n",
    "               wandb.log({'val_atom_level_accuracy_step':atom_level_accuracy,  'global_step':global_step})\n",
    "               wandb.log({'val_residual_level_accuracy_step':residual_level_accuracy, 'global_step':global_step})\n",
    "\n",
    "\n",
    "          epoch_loss_record_val.append(np.mean(epoch_loss_val))\n",
    "          epoch_atom_level_accuracy_record_val.append(np.mean(epoch_atom_level_accuracy_val))\n",
    "          epoch_residual_level_accuracy_record_val.append(np.mean(epoch_residual_level_accuracy_val))\n",
    "\n",
    "          wandb.log({'val_loss_epoch':np.mean(epoch_loss_val), 'global_step':global_step})\n",
    "          wandb.log({'val_atom_level_accuracy_epoch':np.mean(epoch_atom_level_accuracy_val), 'global_step':global_step})\n",
    "          wandb.log({'val_residual_level_accuracy_epoch':np.mean(epoch_residual_level_accuracy_val), 'global_step':global_step})\n",
    "\n",
    "wandb.finish()\n",
    "print(\"Finished training.\")\n",
    "\n",
    "torch.save(model.state_dict(), '/work3/s230027/DL/result/gcpnet/gcpnet_CVsetup1_model_size1_epoch20.pth')\n",
    "print('epoch_residual_level_accuracy_record_train',epoch_residual_level_accuracy_record_train)\n",
    "print('epoch_residual_level_accuracy_record_val',epoch_residual_level_accuracy_record_val)\n",
    "print('epoch_loss_record_train',epoch_loss_record_train)\n",
    "print('epoch_loss_record_val',epoch_loss_record_val)\n",
    "\n",
    "node_acc_results = np.concatenate([ [np.array(epoch_residual_level_accuracy_record_train)], [np.array(epoch_residual_level_accuracy_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/gcpnet/CVsetup1_residual_acc_results.csv\", node_acc_results, delimiter=',', comments=\"\", fmt='%s')\n",
    "\n",
    "loss_results = np.concatenate([[np.array(epoch_loss_record_train)], [np.array(epoch_loss_record_val)] ])\n",
    "np.savetxt(\"/work3/s230027/DL/result/gcpnet/CVsetup1_loss_results.csv\", loss_results, delimiter=',', comments=\"\", fmt='%s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
